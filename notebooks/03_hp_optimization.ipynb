{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 \u2014 Hyperparameter Optimization Analysis\n",
        "\n",
        "Goals:\n",
        "- Analyze outcomes of 5 key experiments from `logs/experiments.jsonl`\n",
        "- Diagnose early stopping behavior (best epoch range 4-11)\n",
        "- Recommend sweep ranges for `lr`, `dropout`, `batch_size`, `weight_decay`\n",
        "- Assess regularization ideas for this regression + online inference setup\n",
        "\n",
        "Artifacts are in `notebooks/artifacts/03_hp_optimization/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ROOT = Path('..') if Path.cwd().name == 'notebooks' else Path('.')\n",
        "ART = ROOT / 'notebooks' / 'artifacts' / '03_hp_optimization'\n",
        "ART.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Build or refresh optimization artifacts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Uncomment to recompute analysis artifacts\n",
        "# import subprocess, sys\n",
        "# subprocess.run([sys.executable, str(ROOT / 'notebooks' / 'run_03_hp_optimization.py')], check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Experiment summary (5 runs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "best_runs = pd.read_csv(ART / 'experiment_comparison_best_runs.csv')\n",
        "all_runs = pd.read_csv(ART / 'experiment_comparison_all_runs.csv')\n",
        "\n",
        "display(best_runs[['config', 'model', 'val_score_avg', 'val_score_t0', 'val_score_t1', 'best_epoch', 'epochs_trained', 'lr', 'dropout', 'batch_size', 'weight_decay']])\n",
        "\n",
        "print('All runs (including repeated gru_derived_v1):')\n",
        "display(all_runs[['config', 'val_score_avg', 'best_epoch', 'epochs_trained', 'timestamp']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Stopping dynamics (curve proxy)\n",
        "\n",
        "Per-epoch train/val history is not persisted for these runs, so this notebook uses `best_epoch`, `epochs_trained`, and score outcomes as curve proxies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "stopping = pd.read_csv(ART / 'stopping_dynamics_summary.csv')\n",
        "display(stopping)\n",
        "\n",
        "img = plt.imread(ART / 'stopping_dynamics.png')\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.title('Stopping dynamics overview')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "note = Path(ART / 'curve_data_availability.txt').read_text(encoding='utf-8')\n",
        "print(note)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Recommended hyperparameter sweep ranges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "ranges = json.load(open(ART / 'recommended_hp_ranges.json', 'r', encoding='utf-8'))\n",
        "ranges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "sensitivity = pd.read_csv(ART / 'hyperparam_sensitivity_coarse.csv')\n",
        "reg = pd.read_csv(ART / 'regularization_recommendations.csv')\n",
        "\n",
        "display(sensitivity)\n",
        "display(reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Practical interpretation\n",
        "\n",
        "- Early peaks at epochs 4-11 are consistent across all tested configs, including weaker ones.\n",
        "- This pattern is expected for this dataset size and objective, and indicates quick fitting plus quick overfit.\n",
        "- Keep early stopping, constrain epoch budget, and focus on regularization + tighter HP ranges rather than larger models.\n",
        "- Feature dropout / modestly stronger regularization is more promising than label smoothing."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}