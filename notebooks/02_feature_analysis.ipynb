{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 \u2014 Feature Analysis (Derived Features + t1 Deep Dive)\n",
        "\n",
        "Goals:\n",
        "- Analyze contribution of the 10 current derived features (`src/data/preprocessing.py`)\n",
        "- Deep analysis of `t1` predictability and cross-feature interactions\n",
        "- Evaluate candidate new derived signals (rolling stats, ROC, volatility)\n",
        "\n",
        "Artifacts are saved in `notebooks/artifacts/02_feature_analysis/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ROOT = Path('..') if Path.cwd().name == 'notebooks' else Path('.')\n",
        "ART = ROOT / 'notebooks' / 'artifacts' / '02_feature_analysis'\n",
        "ART.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load datasets (required context check)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "train = pd.read_parquet(ROOT / 'datasets' / 'train.parquet')\n",
        "valid = pd.read_parquet(ROOT / 'datasets' / 'valid.parquet')\n",
        "\n",
        "print('Train shape:', train.shape)\n",
        "print('Valid shape:', valid.shape)\n",
        "print('Train sequences:', train['seq_ix'].nunique())\n",
        "print('Valid sequences:', valid['seq_ix'].nunique())\n",
        "print('Scored rows train:', int(train['need_prediction'].sum()))\n",
        "print('Scored rows valid:', int(valid['need_prediction'].sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Run full analysis pipeline\n",
        "\n",
        "This script computes all derived-feature rankings, t1 analysis, interaction scans, lag analysis, and candidate feature tests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Uncomment to recompute from scratch\n",
        "# import subprocess, sys\n",
        "# subprocess.run([sys.executable, str(ROOT / 'notebooks' / 'run_02_feature_analysis.py')], check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Derived feature contribution results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "derived_rank = pd.read_csv(ART / 'derived_feature_contribution_rank.csv')\n",
        "perm = pd.read_csv(ART / 'derived_feature_permutation_importance.csv')\n",
        "proxy = json.load(open(ART / 'feature_set_proxy_scores.json', 'r', encoding='utf-8'))\n",
        "\n",
        "display(derived_rank[['feature', 'valid_pearson_t0', 'valid_pearson_t1', 'valid_weightedcorr_t0', 'valid_weightedcorr_t1']])\n",
        "display(perm[['feature', 'delta_avg', 'delta_t0', 'delta_t1']])\n",
        "proxy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].imshow(plt.imread(ART / 'derived_feature_corr_bars.png'))\n",
        "axes[0].axis('off')\n",
        "axes[0].set_title('Derived corr bars')\n",
        "axes[1].imshow(plt.imread(ART / 'derived_feature_permutation_importance.png'))\n",
        "axes[1].axis('off')\n",
        "axes[1].set_title('Permutation importance')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) t1 predictability deep dive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "t1_corr = pd.read_csv(ART / 't1_feature_correlations_42.csv')\n",
        "t1_int = pd.read_csv(ART / 't1_interaction_scan_top10.csv')\n",
        "t1_lag = pd.read_csv(ART / 't1_lag_feature_correlations.csv')\n",
        "t1_mi = pd.read_csv(ART / 't1_mutual_information.csv')\n",
        "\n",
        "print('Top t1 features by |valid pearson|:')\n",
        "display(t1_corr.head(12))\n",
        "print('Top interaction terms for t1:')\n",
        "display(t1_int.head(12))\n",
        "print('Top MI features for t1:')\n",
        "display(t1_mi.head(12))\n",
        "print('Lag correlations (feature[t-lag] vs t1[t]):')\n",
        "display(t1_lag.head(25))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].imshow(plt.imread(ART / 't1_top_feature_corr.png'))\n",
        "axes[0].axis('off')\n",
        "axes[0].set_title('Top t1 feature correlations')\n",
        "axes[1].imshow(plt.imread(ART / 't1_top_interactions.png'))\n",
        "axes[1].axis('off')\n",
        "axes[1].set_title('Top t1 interactions')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Candidate new derived features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "cand = pd.read_csv(ART / 'candidate_new_features_t1_corr.csv')\n",
        "\n",
        "display(cand)\n",
        "\n",
        "img = plt.imread(ART / 'candidate_new_features_t1_corr.png')\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.title('Candidate new derived features vs t1')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Key conclusions\n",
        "\n",
        "- Current derived set helps mainly via spreads and trade-intensity (`spread_2`, `spread_0`, `trade_intensity`).\n",
        "- `t1` remains weak in linear signal; best standalone correlations are low (roughly 0.02-0.04), but interactions add incremental signal.\n",
        "- Strongest new candidate for `t1` from this pass is `spread0_roc1`, then `spread0_roc5`, then short rolling mean of trade intensity.\n",
        "- Next feature-engineering round should prioritize temporal derivatives and rolling volatility-style channels."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}