{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 07: Attention Model Seed Expansion\n",
        "\n",
        "Train combined-loss attention models (gru_attention_clean_v1) with seeds 45-52.\n",
        "We currently have 3 seeds (42-44). This expands to 11 total for better\n",
        "ensemble diversity and selection.\n",
        "\n",
        "**Config**: `configs/gru_attention_clean_v1.yaml`\n",
        "**Estimated time**: ~15-20 min per seed on T4 GPU\n",
        "**Run 2-3 seeds per Kaggle session**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload competition_package.zip to Kaggle, then run:\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Adjust path if needed\n",
        "ZIP_PATH = '/kaggle/input/competition-package/competition_package.zip'\n",
        "WORK_DIR = '/kaggle/working/competition_package'\n",
        "\n",
        "if not os.path.exists(WORK_DIR):\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
        "        z.extractall('/kaggle/working/')\n",
        "\n",
        "os.chdir(WORK_DIR)\n",
        "print(f'Working directory: {os.getcwd()}')\n",
        "print(f'Files: {os.listdir(\".\")[:10]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration: which seeds to train in this session\n",
        "# Adjust SEEDS list per session (2-3 seeds per session)\n",
        "# Session 1: [45, 46, 47]\n",
        "# Session 2: [48, 49, 50]\n",
        "# Session 3: [51, 52]\n",
        "\n",
        "SEEDS = [45, 46, 47]  # <-- CHANGE PER SESSION\n",
        "CONFIG = 'configs/gru_attention_clean_v1.yaml'\n",
        "OUTPUT_DIR = '/kaggle/working'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import shutil\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'Training attention model seed {seed}')\n",
        "    print(f'{\"=\"*60}')\n",
        "    \n",
        "    seed_dir = f'{OUTPUT_DIR}/attn_s{seed}'\n",
        "    \n",
        "    # Train\n",
        "    ret = os.system(\n",
        "        f'python scripts/train.py '\n",
        "        f'--config {CONFIG} '\n",
        "        f'--seed {seed} '\n",
        "        f'--device auto'\n",
        "    )\n",
        "    \n",
        "    if ret != 0:\n",
        "        print(f'ERROR: Training failed for seed {seed}')\n",
        "        continue\n",
        "    \n",
        "    # Find the checkpoint (saved as logs/<config_name>_seed<N>.pt)\n",
        "    config_name = 'gru_attention_clean_v1'\n",
        "    ckpt_name = f'{config_name}_seed{seed}.pt'\n",
        "    ckpt_path = f'logs/{ckpt_name}'\n",
        "    \n",
        "    if not os.path.exists(ckpt_path):\n",
        "        print(f'ERROR: Checkpoint not found at {ckpt_path}')\n",
        "        continue\n",
        "    \n",
        "    # Strip checkpoint (keep only model_state_dict)\n",
        "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
        "    slim = {'model_state_dict': ckpt['model_state_dict']}\n",
        "    slim_path = f'{OUTPUT_DIR}/attn_clean_seed{seed}.pt'\n",
        "    torch.save(slim, slim_path)\n",
        "    slim_size = os.path.getsize(slim_path) / 1024 / 1024\n",
        "    print(f'Saved slim checkpoint: {slim_path} ({slim_size:.1f} MB)')\n",
        "    \n",
        "    # Copy normalizer\n",
        "    norm_src = f'logs/normalizer_{config_name}_seed{seed}.npz'\n",
        "    norm_dst = f'{OUTPUT_DIR}/normalizer_attn_clean_seed{seed}.npz'\n",
        "    if os.path.exists(norm_src):\n",
        "        shutil.copy(norm_src, norm_dst)\n",
        "        print(f'Saved normalizer: {norm_dst}')\n",
        "    else:\n",
        "        print(f'WARNING: Normalizer not found at {norm_src}')\n",
        "    \n",
        "    # Report val score from training history\n",
        "    val_score = ckpt.get('best_score', 'N/A')\n",
        "    best_epoch = ckpt.get('best_epoch', 'N/A')\n",
        "    print(f'Val score: {val_score}, Best epoch: {best_epoch}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all output files for download\n",
        "print('\\nOutput files to download:')\n",
        "print('=' * 60)\n",
        "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
        "    if f.endswith('.pt') or f.endswith('.npz'):\n",
        "        size = os.path.getsize(f'{OUTPUT_DIR}/{f}') / 1024 / 1024\n",
        "        print(f'  {f} ({size:.1f} MB)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-Training Checklist\n",
        "\n",
        "1. Download all `attn_clean_seed{N}.pt` and `normalizer_attn_clean_seed{N}.npz` files\n",
        "2. Place in `C:\\Users\\Vincent\\Downloads\\` for use with `build_mixed_ensemble.py`\n",
        "3. Run local validation with `validate_ensemble_local.py` to score new models\n",
        "4. Update CLAUDE.md with val scores for new seeds"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
