{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Mount Drive, download data from Kaggle\n",
    "import os, json\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.makedirs('/content/drive/MyDrive/wunderfund', exist_ok=True)\n",
    "\n",
    "!pip install -q kaggle==1.6.14 --force-reinstall\n",
    "os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "    json.dump({\"username\": \"vincentvdo6\", \"key\": \"KGAT_17c43012d9e77edf2c183a25acb1489b\"}, f)\n",
    "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
    "\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "!kaggle datasets download -d vincentvdo6/wunderfund-predictorium -p /content/data/ --force\n",
    "!unzip -o -q /content/data/wunderfund-predictorium.zip -d /content/data/\n",
    "!ls /content/data/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup — clone repo, link data\n",
    "import os, subprocess\n",
    "REPO = \"/content/competition_package\"\n",
    "\n",
    "os.chdir(\"/content\")\n",
    "os.system(f\"rm -rf {REPO}\")\n",
    "os.system(f\"git clone https://github.com/vincentvdo6/competition_package.git {REPO}\")\n",
    "os.chdir(REPO)\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "os.system('ln -sf /content/data/train.parquet datasets/train.parquet')\n",
    "os.system('ln -sf /content/data/valid.parquet datasets/valid.parquet')\n",
    "\n",
    "assert os.path.exists(\"datasets/train.parquet\")\n",
    "assert os.path.exists(\"datasets/valid.parquet\")\n",
    "print(\"Commit:\", subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"], text=True).strip())\n",
    "print(f\"GPU: {os.popen('nvidia-smi --query-gpu=name --format=csv,noheader').read().strip()}\")\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Train baseline_match × 3 seeds (CONTROL for all comparisons)\n",
    "# h=64, 3 layers, 32 raw features, linear output — matches official baseline arch\n",
    "import os, subprocess\n",
    "os.chdir(\"/content/competition_package\")\n",
    "\n",
    "for seed in [42, 43, 44]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f'Training gru_baseline_match_v1 seed {seed}')\n",
    "    print(f\"{'='*60}\", flush=True)\n",
    "    p = subprocess.Popen(\n",
    "        ['python', '-u', 'scripts/train.py',\n",
    "         '--config', 'configs/gru_baseline_match_v1.yaml',\n",
    "         '--seed', str(seed), '--device', 'cuda'],\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        text=True, bufsize=1\n",
    "    )\n",
    "    for line in p.stdout:\n",
    "        print(line, end='')\n",
    "    rc = p.wait()\n",
    "    if rc != 0:\n",
    "        print(f'ERROR: seed {seed} failed with exit code {rc}')\n",
    "\n",
    "print('\\nBaseline match training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: WINDOWED INFERENCE EVAL — key test!\n",
    "# The official baseline uses 100-step context window, not step-by-step.\n",
    "# Test if windowed inference improves our baseline_match checkpoints.\n",
    "import os, subprocess, glob\n",
    "os.chdir(\"/content/competition_package\")\n",
    "\n",
    "for pt in sorted(glob.glob('logs/gru_baseline_match_v1_seed*.pt')):\n",
    "    if '_epoch' in pt:\n",
    "        continue\n",
    "    seed = pt.split('seed')[1].split('.')[0]\n",
    "    npz = f'logs/normalizer_gru_baseline_match_v1_seed{seed}.npz'\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f'Windowed eval: seed {seed}')\n",
    "    print(f\"{'='*60}\", flush=True)\n",
    "    p = subprocess.Popen(\n",
    "        ['python', '-u', 'scripts/eval_windowed.py',\n",
    "         '--checkpoint', pt, '--normalizer', npz,\n",
    "         '--window', '100'],\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        text=True, bufsize=1\n",
    "    )\n",
    "    for line in p.stdout:\n",
    "        print(line, end='')\n",
    "    rc = p.wait()\n",
    "    if rc != 0:\n",
    "        print(f'ERROR: windowed eval failed with exit code {rc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Train chrono v2 × 3 seeds (FIXED: bias_hh only, T=10)\n",
    "import os, subprocess\n",
    "os.chdir(\"/content/competition_package\")\n",
    "\n",
    "for seed in [42, 43, 44]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f'Training gru_v2_chrono seed {seed}')\n",
    "    print(f\"{'='*60}\", flush=True)\n",
    "    p = subprocess.Popen(\n",
    "        ['python', '-u', 'scripts/train.py',\n",
    "         '--config', 'configs/gru_v2_chrono.yaml',\n",
    "         '--seed', str(seed), '--device', 'cuda'],\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        text=True, bufsize=1\n",
    "    )\n",
    "    for line in p.stdout:\n",
    "        print(line, end='')\n",
    "    rc = p.wait()\n",
    "    if rc != 0:\n",
    "        print(f'ERROR: seed {seed} failed with exit code {rc}')\n",
    "\n",
    "print('\\nChrono v2 training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Train aug v2 × 3 seeds (FIXED: scale 0.95-1.05)\n",
    "import os, subprocess\n",
    "os.chdir(\"/content/competition_package\")\n",
    "\n",
    "for seed in [42, 43, 44]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f'Training gru_v2_aug seed {seed}')\n",
    "    print(f\"{'='*60}\", flush=True)\n",
    "    p = subprocess.Popen(\n",
    "        ['python', '-u', 'scripts/train.py',\n",
    "         '--config', 'configs/gru_v2_aug.yaml',\n",
    "         '--seed', str(seed), '--device', 'cuda'],\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        text=True, bufsize=1\n",
    "    )\n",
    "    for line in p.stdout:\n",
    "        print(line, end='')\n",
    "    rc = p.wait()\n",
    "    if rc != 0:\n",
    "        print(f'ERROR: seed {seed} failed with exit code {rc}')\n",
    "\n",
    "print('\\nAugmentation v2 training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train SWA v2 × 3 seeds (FIXED: start_epoch=15)\n",
    "import os, subprocess\n",
    "os.chdir(\"/content/competition_package\")\n",
    "\n",
    "for seed in [42, 43, 44]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f'Training gru_v2_swa seed {seed}')\n",
    "    print(f\"{'='*60}\", flush=True)\n",
    "    p = subprocess.Popen(\n",
    "        ['python', '-u', 'scripts/train.py',\n",
    "         '--config', 'configs/gru_v2_swa.yaml',\n",
    "         '--seed', str(seed), '--device', 'cuda'],\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        text=True, bufsize=1\n",
    "    )\n",
    "    for line in p.stdout:\n",
    "        print(line, end='')\n",
    "    rc = p.wait()\n",
    "    if rc != 0:\n",
    "        print(f'ERROR: seed {seed} failed with exit code {rc}')\n",
    "\n",
    "print('\\nSWA v2 training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Evaluate ALL configs\n",
    "import os, torch, glob\n",
    "os.chdir(\"/content/competition_package\")\n",
    "\n",
    "configs_to_eval = [\n",
    "    ('baseline_match', 'gru_baseline_match_v1'),\n",
    "    ('chrono_v2', 'gru_v2_chrono'),\n",
    "    ('aug_v2', 'gru_v2_aug'),\n",
    "    ('swa_v2', 'gru_v2_swa'),\n",
    "]\n",
    "\n",
    "all_results = {}\n",
    "for label, prefix in configs_to_eval:\n",
    "    print(f'\\n--- {label} ({prefix}) ---')\n",
    "    scores = []\n",
    "    for pt in sorted(glob.glob(f'logs/{prefix}_seed*.pt')):\n",
    "        if '_epoch' in pt:\n",
    "            continue\n",
    "        ckpt = torch.load(pt, map_location='cpu', weights_only=False)\n",
    "        score = float(ckpt.get('best_score', 0.0))\n",
    "        epoch = ckpt.get('best_epoch', 0)\n",
    "        print(f'  {os.path.basename(pt)}: val={score:.4f} (best epoch {epoch})')\n",
    "        scores.append(score)\n",
    "    if scores:\n",
    "        mean_score = sum(scores) / len(scores)\n",
    "        print(f'  Mean: {mean_score:.4f}, Min: {min(scores):.4f}, Max: {max(scores):.4f}')\n",
    "        all_results[label] = {'scores': scores, 'mean': mean_score}\n",
    "    else:\n",
    "        print('  No checkpoints found!')\n",
    "        all_results[label] = {'scores': [], 'mean': 0}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print('KILL TEST SUMMARY')\n",
    "print(f\"{'='*60}\")\n",
    "bm = all_results.get('baseline_match', {})\n",
    "bm_mean = bm.get('mean', 0)\n",
    "bm_scores = bm.get('scores', [])\n",
    "print(f'baseline_match mean: {bm_mean:.4f} (control)')\n",
    "print()\n",
    "\n",
    "for label in ['chrono_v2', 'aug_v2', 'swa_v2']:\n",
    "    r = all_results.get(label, {})\n",
    "    r_mean = r.get('mean', 0)\n",
    "    r_scores = r.get('scores', [])\n",
    "    delta = r_mean - bm_mean\n",
    "    n_pos = sum(1 for s, b in zip(r_scores, bm_scores) if s > b) if r_scores and bm_scores else 0\n",
    "    print(f'{label}: mean={r_mean:.4f}, delta={delta:+.4f}, positive={n_pos}/{len(r_scores)}')\n",
    "    if delta >= 0.0010 and n_pos >= 2:\n",
    "        print(f'  PASS!')\n",
    "    elif delta > 0:\n",
    "        print(f'  MARGINAL')\n",
    "    else:\n",
    "        print(f'  FAIL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Training curves\n",
    "import json, glob, os\n",
    "os.chdir(\"/content/competition_package\")\n",
    "\n",
    "for prefix in ['gru_baseline_match_v1', 'gru_v2_chrono', 'gru_v2_aug', 'gru_v2_swa']:\n",
    "    print(f'\\n--- {prefix} ---')\n",
    "    for hist_file in sorted(glob.glob(f'logs/training_history_{prefix}*.json')):\n",
    "        with open(hist_file) as f:\n",
    "            hist = json.load(f)\n",
    "        name = os.path.basename(hist_file).replace('training_history_', '').replace('.json', '')\n",
    "        scores = [s['avg'] for s in hist['val_scores']]\n",
    "        t0s = [s['t0'] for s in hist['val_scores']]\n",
    "        t1s = [s['t1'] for s in hist['val_scores']]\n",
    "        bi = scores.index(max(scores))\n",
    "        print(f'  {name}: {len(scores)} epochs, best={max(scores):.4f} @{bi+1}, '\n",
    "              f't0={t0s[bi]:.4f} t1={t1s[bi]:.4f} ratio={t0s[bi]/max(t1s[bi],1e-8):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Strip checkpoints + zip + save to Drive\n",
    "import os, torch, glob, shutil\n",
    "os.chdir(\"/content/competition_package\")\n",
    "os.makedirs('logs/slim', exist_ok=True)\n",
    "\n",
    "for prefix in ['gru_baseline_match_v1', 'gru_v2_chrono', 'gru_v2_aug', 'gru_v2_swa']:\n",
    "    for pt in sorted(glob.glob(f'logs/{prefix}_*.pt')):\n",
    "        if '_epoch' in pt:\n",
    "            continue\n",
    "        ckpt = torch.load(pt, map_location='cpu', weights_only=False)\n",
    "        slim = {\n",
    "            'model_state_dict': ckpt['model_state_dict'],\n",
    "            'config': ckpt.get('config', {}),\n",
    "            'best_score': ckpt.get('best_score', None),\n",
    "        }\n",
    "        out = f'logs/slim/{os.path.basename(pt)}'\n",
    "        torch.save(slim, out)\n",
    "        orig = os.path.getsize(pt) / 1e6\n",
    "        new = os.path.getsize(out) / 1e6\n",
    "        print(f'{os.path.basename(pt)}: {orig:.1f}MB -> {new:.1f}MB')\n",
    "    for npz in sorted(glob.glob(f'logs/normalizer_{prefix}*.npz')):\n",
    "        shutil.copy(npz, f'logs/slim/{os.path.basename(npz)}')\n",
    "        print(f'Copied {os.path.basename(npz)}')\n",
    "\n",
    "print(f'\\n--- logs/slim/ ({len(os.listdir(\"logs/slim\"))} files) ---')\n",
    "total_mb = 0\n",
    "for f in sorted(os.listdir('logs/slim')):\n",
    "    sz = os.path.getsize(f'logs/slim/{f}') / 1e6\n",
    "    total_mb += sz\n",
    "    print(f'  {f}: {sz:.1f}MB')\n",
    "print(f'  Total: {total_mb:.1f}MB')\n",
    "\n",
    "shutil.make_archive('/content/baseline_match_v2', 'zip', '/content/competition_package/logs/slim')\n",
    "sz = os.path.getsize('/content/baseline_match_v2.zip') / 1e6\n",
    "print(f'\\nbaseline_match_v2.zip: {sz:.1f}MB')\n",
    "\n",
    "shutil.copy('/content/baseline_match_v2.zip', '/content/drive/MyDrive/wunderfund/baseline_match_v2.zip')\n",
    "print('Saved to Drive!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
