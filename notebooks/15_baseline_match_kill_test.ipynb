{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Mount Drive, download data from Kaggle\n",
    "import os, json\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.makedirs('/content/drive/MyDrive/wunderfund', exist_ok=True)\n",
    "\n",
    "!pip install -q kaggle==1.6.14 --force-reinstall\n",
    "os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "    json.dump({\"username\": \"vincentvdo6\", \"key\": \"KGAT_17c43012d9e77edf2c183a25acb1489b\"}, f)\n",
    "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
    "\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "!kaggle datasets download -d vincentvdo6/wunderfund-predictorium -p /content/data/ --force\n",
    "!unzip -o -q /content/data/wunderfund-predictorium.zip -d /content/data/\n",
    "!ls /content/data/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup — clone repo, link data\n",
    "import os, subprocess\n",
    "REPO = \"/content/competition_package\"\n",
    "\n",
    "os.chdir(\"/content\")\n",
    "os.system(f\"rm -rf {REPO}\")\n",
    "os.system(f\"git clone https://github.com/vincentvdo6/competition_package.git {REPO}\")\n",
    "os.chdir(REPO)\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "os.system('ln -sf /content/data/train.parquet datasets/train.parquet')\n",
    "os.system('ln -sf /content/data/valid.parquet datasets/valid.parquet')\n",
    "\n",
    "assert os.path.exists(\"datasets/train.parquet\")\n",
    "assert os.path.exists(\"datasets/valid.parquet\")\n",
    "print(\"Commit:\", subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"], text=True).strip())\n",
    "print(f\"GPU: {os.popen('nvidia-smi --query-gpu=name --format=csv,noheader').read().strip()}\")\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Train baseline_match × 3 seeds (CONTROL — already validated at mean 0.2663)\n# SKIP if already trained in Round 1/2 — uncomment to retrain fresh control\nimport os, subprocess\nos.chdir(\"/content/competition_package\")\n\nfor seed in [42, 43, 44]:\n    print(f\"\\n{'='*60}\")\n    print(f'Training gru_baseline_match_v1 seed {seed}')\n    print(f\"{'='*60}\", flush=True)\n    p = subprocess.Popen(\n        ['python', '-u', 'scripts/train.py',\n         '--config', 'configs/gru_baseline_match_v1.yaml',\n         '--seed', str(seed), '--device', 'cuda'],\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n        text=True, bufsize=1\n    )\n    for line in p.stdout:\n        print(line, end='')\n    rc = p.wait()\n    if rc != 0:\n        print(f'ERROR: seed {seed} failed with exit code {rc}')\n\nprint('\\nBaseline match training done!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Train MSE loss × 3 seeds (ABLATION 1: loss only change)\n# Hypothesis: official baseline likely uses simple MSE, not weighted MSE\nimport os, subprocess\nos.chdir(\"/content/competition_package\")\n\nfor seed in [42, 43, 44]:\n    print(f\"\\n{'='*60}\")\n    print(f'Training gru_baseline_mse seed {seed}')\n    print(f\"{'='*60}\", flush=True)\n    p = subprocess.Popen(\n        ['python', '-u', 'scripts/train.py',\n         '--config', 'configs/gru_baseline_mse.yaml',\n         '--seed', str(seed), '--device', 'cuda'],\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n        text=True, bufsize=1\n    )\n    for line in p.stdout:\n        print(line, end='')\n    rc = p.wait()\n    if rc != 0:\n        print(f'ERROR: seed {seed} failed with exit code {rc}')\n\nprint('\\nMSE loss training done!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Train raw features × 3 seeds (ABLATION 2: no normalization)\n# Hypothesis: official baseline feeds raw features, normalization may hurt\nimport os, subprocess\nos.chdir(\"/content/competition_package\")\n\nfor seed in [42, 43, 44]:\n    print(f\"\\n{'='*60}\")\n    print(f'Training gru_baseline_raw seed {seed}')\n    print(f\"{'='*60}\", flush=True)\n    p = subprocess.Popen(\n        ['python', '-u', 'scripts/train.py',\n         '--config', 'configs/gru_baseline_raw.yaml',\n         '--seed', str(seed), '--device', 'cuda'],\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n        text=True, bufsize=1\n    )\n    for line in p.stdout:\n        print(line, end='')\n    rc = p.wait()\n    if rc != 0:\n        print(f'ERROR: seed {seed} failed with exit code {rc}')\n\nprint('\\nRaw features training done!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Train MSE+raw combined × 3 seeds (ABLATION 3: both changes)\n# Only run if at least one of MSE or raw passes, otherwise skip\nimport os, subprocess\nos.chdir(\"/content/competition_package\")\n\nfor seed in [42, 43, 44]:\n    print(f\"\\n{'='*60}\")\n    print(f'Training gru_baseline_mse_raw seed {seed}')\n    print(f\"{'='*60}\", flush=True)\n    p = subprocess.Popen(\n        ['python', '-u', 'scripts/train.py',\n         '--config', 'configs/gru_baseline_mse_raw.yaml',\n         '--seed', str(seed), '--device', 'cuda'],\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n        text=True, bufsize=1\n    )\n    for line in p.stdout:\n        print(line, end='')\n    rc = p.wait()\n    if rc != 0:\n        print(f'ERROR: seed {seed} failed with exit code {rc}')\n\nprint('\\nMSE+raw training done!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Evaluate ALL Round 3 configs\nimport os, torch, glob\nos.chdir(\"/content/competition_package\")\n\nconfigs_to_eval = [\n    ('baseline_match', 'gru_baseline_match_v1'),\n    ('mse', 'gru_baseline_mse'),\n    ('raw', 'gru_baseline_raw'),\n    ('mse_raw', 'gru_baseline_mse_raw'),\n]\n\nall_results = {}\nfor label, prefix in configs_to_eval:\n    print(f'\\n--- {label} ({prefix}) ---')\n    scores = []\n    for pt in sorted(glob.glob(f'logs/{prefix}_seed*.pt')):\n        if '_epoch' in pt:\n            continue\n        ckpt = torch.load(pt, map_location='cpu', weights_only=False)\n        score = float(ckpt.get('best_score', 0.0))\n        epoch = ckpt.get('best_epoch', 0)\n        print(f'  {os.path.basename(pt)}: val={score:.4f} (best epoch {epoch})')\n        scores.append(score)\n    if scores:\n        mean_score = sum(scores) / len(scores)\n        print(f'  Mean: {mean_score:.4f}, Min: {min(scores):.4f}, Max: {max(scores):.4f}')\n        all_results[label] = {'scores': scores, 'mean': mean_score}\n    else:\n        print('  No checkpoints found!')\n        all_results[label] = {'scores': [], 'mean': 0}\n\nprint(f\"\\n{'='*60}\")\nprint('ROUND 3 KILL TEST SUMMARY')\nprint(f\"{'='*60}\")\nbm = all_results.get('baseline_match', {})\nbm_mean = bm.get('mean', 0)\nbm_scores = bm.get('scores', [])\nprint(f'baseline_match mean: {bm_mean:.4f} (control)')\nprint()\n\nfor label in ['mse', 'raw', 'mse_raw']:\n    r = all_results.get(label, {})\n    r_mean = r.get('mean', 0)\n    r_scores = r.get('scores', [])\n    delta = r_mean - bm_mean\n    n_pos = sum(1 for s, b in zip(r_scores, bm_scores) if s > b) if r_scores and bm_scores else 0\n    print(f'{label}: mean={r_mean:.4f}, delta={delta:+.4f}, positive={n_pos}/{len(r_scores)}')\n    if delta >= 0.0010 and n_pos >= 2:\n        print(f'  >>> PASS! <<<')\n    elif delta > 0:\n        print(f'  MARGINAL (positive but below threshold)')\n    else:\n        print(f'  FAIL')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Training curves for Round 3\nimport json, glob, os\nos.chdir(\"/content/competition_package\")\n\nfor prefix in ['gru_baseline_match_v1', 'gru_baseline_mse', 'gru_baseline_raw', 'gru_baseline_mse_raw']:\n    print(f'\\n--- {prefix} ---')\n    for hist_file in sorted(glob.glob(f'logs/training_history_{prefix}*.json')):\n        with open(hist_file) as f:\n            hist = json.load(f)\n        name = os.path.basename(hist_file).replace('training_history_', '').replace('.json', '')\n        scores = [s['avg'] for s in hist['val_scores']]\n        t0s = [s['t0'] for s in hist['val_scores']]\n        t1s = [s['t1'] for s in hist['val_scores']]\n        bi = scores.index(max(scores))\n        print(f'  {name}: {len(scores)} epochs, best={max(scores):.4f} @{bi+1}, '\n              f't0={t0s[bi]:.4f} t1={t1s[bi]:.4f} ratio={t0s[bi]/max(t1s[bi],1e-8):.2f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Strip checkpoints + zip + save to Drive\nimport os, torch, glob, shutil\nos.chdir(\"/content/competition_package\")\nos.makedirs('logs/slim', exist_ok=True)\n\nfor prefix in ['gru_baseline_match_v1', 'gru_baseline_mse', 'gru_baseline_raw', 'gru_baseline_mse_raw']:\n    for pt in sorted(glob.glob(f'logs/{prefix}_*.pt')):\n        if '_epoch' in pt:\n            continue\n        ckpt = torch.load(pt, map_location='cpu', weights_only=False)\n        slim = {\n            'model_state_dict': ckpt['model_state_dict'],\n            'config': ckpt.get('config', {}),\n            'best_score': ckpt.get('best_score', None),\n        }\n        out = f'logs/slim/{os.path.basename(pt)}'\n        torch.save(slim, out)\n        orig = os.path.getsize(pt) / 1e6\n        new = os.path.getsize(out) / 1e6\n        print(f'{os.path.basename(pt)}: {orig:.1f}MB -> {new:.1f}MB')\n    for npz in sorted(glob.glob(f'logs/normalizer_{prefix}*.npz')):\n        shutil.copy(npz, f'logs/slim/{os.path.basename(npz)}')\n        print(f'Copied {os.path.basename(npz)}')\n\nprint(f'\\n--- logs/slim/ ({len(os.listdir(\"logs/slim\"))} files) ---')\ntotal_mb = 0\nfor f in sorted(os.listdir('logs/slim')):\n    sz = os.path.getsize(f'logs/slim/{f}') / 1e6\n    total_mb += sz\n    print(f'  {f}: {sz:.1f}MB')\nprint(f'  Total: {total_mb:.1f}MB')\n\nshutil.make_archive('/content/baseline_match_r3', 'zip', '/content/competition_package/logs/slim')\nsz = os.path.getsize('/content/baseline_match_r3.zip') / 1e6\nprint(f'\\nbaseline_match_r3.zip: {sz:.1f}MB')\n\nshutil.copy('/content/baseline_match_r3.zip', '/content/drive/MyDrive/wunderfund/baseline_match_r3.zip')\nprint('Saved to Drive!')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}