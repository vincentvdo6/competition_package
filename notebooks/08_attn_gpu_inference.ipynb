{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 08: Attention Model GPU Inference for Local Validation\n",
    "\n",
    "Runs online inference for 8 new attention models (seeds 45-52) on GPU.\n",
    "Outputs .npz prediction caches compatible with `validate_ensemble_local.py`.\n",
    "\n",
    "**Key optimization**: Batches all 1,444 sequences at each timestep\n",
    "(1,000 iterations vs 1,444,000). ~1-2 min/model on GPU vs ~25 min on CPU.\n",
    "\n",
    "**Requires**: Checkpoint zips from notebook 07 sessions uploaded as dataset(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "REPO = \"/kaggle/working/competition_package\"\n",
    "os.chdir(\"/kaggle/working\")\n",
    "os.system(f\"rm -rf {REPO}\")\n",
    "os.system(f\"git clone https://github.com/vincentvdo6/competition_package.git {REPO}\")\n",
    "os.chdir(REPO)\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "os.system(\"ln -sf /kaggle/input/wunderfund-predictorium/valid.parquet datasets/valid.parquet\")\n",
    "print(\"Commit:\", subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"], text=True).strip())\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Configuration: dataset directory (Kaggle auto-extracted the zip)\nCKPT_DIR = '/kaggle/input/inference-training'\n\nVALID_PATH = 'datasets/valid.parquet'\nOUTPUT_DIR = '/kaggle/working'\n\n# Models to run inference for: name -> (ckpt_file, norm_file, train_val)\nMODELS = {\n    'attn_nb07_s45': ('attn_clean_seed45.pt', 'normalizer_attn_clean_seed45.npz', 0.2599),\n    'attn_nb07_s46': ('attn_clean_seed46.pt', 'normalizer_attn_clean_seed46.npz', 0.2659),\n    'attn_nb07_s47': ('attn_clean_seed47.pt', 'normalizer_attn_clean_seed47.npz', 0.2598),\n    'attn_nb07_s48': ('attn_clean_seed48.pt', 'normalizer_attn_clean_seed48.npz', 0.2706),\n    'attn_nb07_s49': ('attn_clean_seed49.pt', 'normalizer_attn_clean_seed49.npz', 0.2560),\n    'attn_nb07_s50': ('attn_clean_seed50.pt', 'normalizer_attn_clean_seed50.npz', 0.2752),\n    'attn_nb07_s51': ('attn_clean_seed51.pt', 'normalizer_attn_clean_seed51.npz', 0.2600),\n    'attn_nb07_s52': ('attn_clean_seed52.pt', 'normalizer_attn_clean_seed52.npz', 0.2641),\n}\n\nprint(f'Will process {len(MODELS)} attention models from {CKPT_DIR}')\nfor name, (_, _, val) in MODELS.items():\n    print(f'  {name}: train_val={val:.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "import sys, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport torch\nimport yaml\n\nsys.path.insert(0, REPO)\nfrom src.models.gru_attention import GRUAttentionModel\nfrom src.data.preprocessing import DerivedFeatureBuilder, Normalizer\nfrom utils import weighted_pearson_correlation\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name()}')\n\nwith open('configs/gru_attention_clean_v1.yaml') as f:\n    ATTN_CONFIG = yaml.safe_load(f)\nprint(f'Config loaded: hidden={ATTN_CONFIG[\"model\"][\"hidden_size\"]}, '\n      f'heads={ATTN_CONFIG[\"model\"][\"attention_heads\"]}, '\n      f'window={ATTN_CONFIG[\"model\"][\"attention_window\"]}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and reshape validation data\n",
    "print(f'Loading {VALID_PATH}...')\n",
    "df = pd.read_parquet(VALID_PATH)\n",
    "n_rows = len(df)\n",
    "n_seqs = df['seq_ix'].nunique()\n",
    "seq_len = n_rows // n_seqs\n",
    "print(f'  {n_rows} rows, {n_seqs} sequences, {seq_len} steps/seq')\n",
    "\n",
    "values = df.values\n",
    "seq_ix_flat = values[:, 0].astype(int)\n",
    "need_pred_flat = values[:, 2].astype(bool)\n",
    "raw_features = values[:, 3:35].astype(np.float32).reshape(n_seqs, seq_len, 32)\n",
    "targets_all = values[:, 35:].astype(np.float64).reshape(n_seqs, seq_len, 2)\n",
    "need_pred_2d = need_pred_flat.reshape(n_seqs, seq_len)\n",
    "unique_seq_ix = seq_ix_flat.reshape(n_seqs, seq_len)[:, 0]\n",
    "\n",
    "# Pre-compute derived features for ALL steps (vectorized)\n",
    "print('Computing derived features...')\n",
    "raw_flat = raw_features.reshape(-1, 32)\n",
    "derived_flat = DerivedFeatureBuilder.compute(raw_flat)\n",
    "features_42_all = np.concatenate([raw_flat, derived_flat], axis=-1).reshape(n_seqs, seq_len, 42)\n",
    "\n",
    "# Build mask for filtering predictions\n",
    "mask = need_pred_2d.flatten()\n",
    "seq_indices_filtered = np.repeat(unique_seq_ix, seq_len)[mask]\n",
    "targets_filtered = targets_all.reshape(-1, 2)[mask]\n",
    "\n",
    "print(f'  Features: {features_42_all.shape}, need_pred rows: {mask.sum()}/{len(mask)}')\n",
    "print('Data ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Run batch-online inference for all 8 attention models\nos.makedirs(f'{OUTPUT_DIR}/predictions', exist_ok=True)\nresults = {}\n\nfor name, (ckpt_file, norm_file, val_score) in MODELS.items():\n    print(f'\\n{\"=\"*60}')\n    print(f'Processing {name} (train val={val_score:.4f})')\n    print(f'{\"=\"*60}')\n\n    ckpt_path = f'{CKPT_DIR}/{ckpt_file}'\n    norm_path = f'{CKPT_DIR}/{norm_file}'\n\n    if not os.path.exists(ckpt_path):\n        print(f'  SKIP: checkpoint not found: {ckpt_path}')\n        continue\n\n    # Load model\n    model = GRUAttentionModel(ATTN_CONFIG)\n    ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n    state_dict = ckpt.get('model_state_dict', ckpt)\n    model.load_state_dict(state_dict)\n    model.to(device)\n    model.eval()\n    normalizer = Normalizer.load(norm_path)\n\n    print(f'  Model loaded ({sum(p.numel() for p in model.parameters()):,} params)')\n\n    # Batch-online inference: all sequences in parallel at each step\n    all_preds = np.zeros((n_seqs, seq_len, 2), dtype=np.float32)\n    hidden = None\n    t0 = time.time()\n\n    with torch.no_grad():\n        for t in range(seq_len):\n            step_features = features_42_all[:, t, :]\n            normed = normalizer.transform(step_features)\n            x = torch.from_numpy(normed).to(device)\n            pred, hidden = model.forward_step(x, hidden)\n            all_preds[:, t, :] = pred.cpu().numpy()\n            if (t + 1) % 200 == 0:\n                print(f'    Step {t+1}/{seq_len} in {time.time()-t0:.1f}s')\n\n    elapsed = time.time() - t0\n    preds_flat = np.clip(all_preds.reshape(-1, 2)[mask], -6, 6)\n\n    # Score\n    t0_score = weighted_pearson_correlation(targets_filtered[:, 0], preds_flat[:, 0])\n    t1_score = weighted_pearson_correlation(targets_filtered[:, 1], preds_flat[:, 1])\n    avg_score = (t0_score + t1_score) / 2.0\n\n    print(f'  {elapsed:.1f}s | t0={t0_score:.4f}  t1={t1_score:.4f}  avg={avg_score:.4f} (train_val={val_score:.4f})')\n\n    # Save .npz (compatible with validate_ensemble_local.py cache format)\n    out_path = f'{OUTPUT_DIR}/predictions/{name}.npz'\n    np.savez_compressed(out_path, preds=preds_flat, targets=targets_filtered, seq_indices=seq_indices_filtered)\n    print(f'  Saved: {out_path}')\n\n    results[name] = {'t0': t0_score, 't1': t1_score, 'avg': avg_score, 'train_val': val_score}\n    del model\n    torch.cuda.empty_cache()\n\nprint(f'\\n{\"=\"*60}')\nprint('ALL MODELS COMPLETE')\nprint(f'{\"=\"*60}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and zip for download\n",
    "print(f'{\"Model\":<20} {\"t0\":>7} {\"t1\":>7} {\"avg\":>7} {\"train_val\":>10}')\n",
    "print('-' * 55)\n",
    "for name, sc in sorted(results.items(), key=lambda x: -x[1]['avg']):\n",
    "    print(f'{name:<20} {sc[\"t0\"]:>7.4f} {sc[\"t1\"]:>7.4f} {sc[\"avg\"]:>7.4f} {sc[\"train_val\"]:>10.4f}')\n",
    "\n",
    "import shutil\n",
    "zip_path = f'{OUTPUT_DIR}/attn_predictions_cache'\n",
    "shutil.make_archive(zip_path, 'zip', f'{OUTPUT_DIR}/predictions')\n",
    "print(f'\\nZip: {zip_path}.zip ({os.path.getsize(zip_path + \".zip\")/1024/1024:.1f} MB)')\n",
    "print('Download from Output tab on the right sidebar.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Post-Inference Checklist\n",
    "\n",
    "1. Download `attn_predictions_cache.zip` from the Output tab\n",
    "2. Extract all `.npz` files into `cache/predictions/` on your local machine\n",
    "3. Run: `python scripts/validate_ensemble_local.py list` to verify cached\n",
    "4. Run: `python scripts/validate_ensemble_local.py compare` to rank all presets\n",
    "5. Run: `python scripts/validate_ensemble_local.py greedy --pool all --max-models 8 --weighted-attn`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}