{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Cell 0: Mount Drive, download data from Kaggle\n",
    "import os, json\n",
    "\n",
    "# Mount Drive for saving outputs\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.makedirs('/content/drive/MyDrive/wunderfund', exist_ok=True)\n",
    "\n",
    "# Install pinned kaggle + set credentials\n",
    "!pip install -q kaggle==1.6.14 --force-reinstall\n",
    "os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "    json.dump({\"username\": \"vincentvdo6\", \"key\": \"KGAT_17c43012d9e77edf2c183a25acb1489b\"}, f)\n",
    "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
    "\n",
    "# Download + unzip dataset\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "!kaggle datasets download -d vincentvdo6/wunderfund-predictorium -p /content/data/ --force\n",
    "!unzip -o -q /content/data/wunderfund-predictorium.zip -d /content/data/\n",
    "!ls /content/data/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Setup — clone repo, link data\n",
    "import os, subprocess\n",
    "REPO = \"/content/competition_package\"\n",
    "\n",
    "os.chdir(\"/content\")\n",
    "os.system(f\"rm -rf {REPO}\")\n",
    "os.system(f\"git clone https://github.com/vincentvdo6/competition_package.git {REPO}\")\n",
    "os.chdir(REPO)\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Link data from Kaggle download\n",
    "os.system('ln -sf /content/data/train.parquet datasets/train.parquet')\n",
    "os.system('ln -sf /content/data/valid.parquet datasets/valid.parquet')\n",
    "\n",
    "# Verify\n",
    "assert os.path.exists(\"datasets/train.parquet\"), \"train.parquet not found!\"\n",
    "assert os.path.exists(\"datasets/valid.parquet\"), \"valid.parquet not found!\"\n",
    "print(\"Commit:\", subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"], text=True).strip())\n",
    "print(f\"GPU: {os.popen('nvidia-smi --query-gpu=name --format=csv,noheader').read().strip()}\")\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-treatment"
   },
   "outputs": [],
   "source": "# Cell 2: Treatment — transformer, 3 seeds\n# Expected: ~5-10 min per seed (50 epochs max, early stopping patience=10)\nimport os, subprocess\nos.chdir(\"/content/competition_package\")\n\nfor seed in [42, 43, 44]:\n    print(f\"\\n{'='*60}\")\n    print(f'Training transformer_v1 seed {seed}')\n    print(f\"{'='*60}\", flush=True)\n    p = subprocess.Popen(\n        ['python', '-u', 'scripts/train.py',\n         '--config', 'configs/transformer_v1.yaml',\n         '--seed', str(seed), '--device', 'cuda'],\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n        text=True, bufsize=1\n    )\n    for line in p.stdout:\n        print(line, end='')\n    rc = p.wait()\n    if rc != 0:\n        print(f'ERROR: seed {seed} failed with exit code {rc}')\n\nprint('\\nTreatment training done!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-control"
   },
   "outputs": [],
   "source": "# Cell 3: (Optional) GRU control for correlation comparison\n# Skip if you already have GRU control scores from previous runs.\n# Control scores from last run: s42=0.2595, s43=0.2652, s44=0.2610\nimport os, subprocess\nos.chdir(\"/content/competition_package\")\n\nfor seed in [42, 43, 44]:\n    print(f\"\\n{'='*60}\")\n    print(f'Training gru_pearson_v1 (control) seed {seed}')\n    print(f\"{'='*60}\", flush=True)\n    p = subprocess.Popen(\n        ['python', '-u', 'scripts/train.py',\n         '--config', 'configs/gru_pearson_v1.yaml',\n         '--seed', str(seed), '--device', 'cuda'],\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n        text=True, bufsize=1\n    )\n    for line in p.stdout:\n        print(line, end='')\n    rc = p.wait()\n    if rc != 0:\n        print(f'ERROR: seed {seed} failed with exit code {rc}')\n\nprint('\\nControl training done!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": "# Cell 4: Evaluate transformer models\n# Kill test: mean val >= 0.2500 (floor for ensemble-viable diversity model)\nimport os, torch, glob\nos.chdir(\"/content/competition_package\")\n\nprint('--- Transformer v1 ---')\nscores = []\nfor pt in sorted(glob.glob('logs/transformer_v1_seed*.pt')):\n    if '_epoch' in pt:\n        continue  # Skip periodic checkpoints\n    ckpt = torch.load(pt, map_location='cpu', weights_only=False)\n    score = float(ckpt.get('best_score', 0.0))\n    epoch = ckpt.get('best_epoch', 0)\n    name = os.path.basename(pt)\n    scores.append(score)\n    print(f'  {name}: val={score:.4f} (best epoch {epoch})')\n\nif scores:\n    mean_score = sum(scores) / len(scores)\n    print(f'\\n{\"=\"*60}')\n    print(f'KILL TEST RESULTS')\n    print(f'{\"=\"*60}')\n    print(f'Mean val: {mean_score:.4f}')\n    print(f'Per-seed: {[\"{:.4f}\".format(s) for s in scores]}')\n    print(f'Min: {min(scores):.4f}, Max: {max(scores):.4f}')\n    print()\n\n    # Kill test: absolute floor for ensemble diversity model\n    if mean_score >= 0.2500:\n        print(f'PASS! Mean {mean_score:.4f} >= 0.2500 floor.')\n        print('Transformer is viable for ensemble diversity.')\n        print('Next: check correlation with GRU (target < 0.92).')\n    elif mean_score >= 0.2400:\n        print(f'MARGINAL: Mean {mean_score:.4f} in [0.24, 0.25) range.')\n        print('May still add value if correlation with GRU is very low (<0.85).')\n    else:\n        print(f'FAIL: Mean {mean_score:.4f} < 0.2400. Not viable.')\nelse:\n    print('ERROR: No checkpoints found! Check training output.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-curves"
   },
   "outputs": [],
   "source": "# Cell 5: Training curves — transformer\nimport json, glob, os\nos.chdir(\"/content/competition_package\")\n\nprint('--- Transformer v1 ---')\nfor hist_file in sorted(glob.glob('logs/training_history_transformer_v1*.json')):\n    with open(hist_file) as f:\n        hist = json.load(f)\n    name = os.path.basename(hist_file).replace('training_history_', '').replace('.json', '')\n    scores = [s['avg'] for s in hist['val_scores']]\n    t0_scores = [s['t0'] for s in hist['val_scores']]\n    t1_scores = [s['t1'] for s in hist['val_scores']]\n    best_idx = scores.index(max(scores))\n    print(f'  {name}:')\n    print(f'    Epochs: {len(scores)}, Best avg: {max(scores):.4f} at epoch {best_idx+1}')\n    print(f'    Best t0: {t0_scores[best_idx]:.4f}, Best t1: {t1_scores[best_idx]:.4f}')\n    print(f'    t0/t1 ratio: {t0_scores[best_idx]/max(t1_scores[best_idx], 1e-8):.2f}')\n    print(f'    Last 5 avg: {[\"{:.4f}\".format(s) for s in scores[-5:]]}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "strip-zip"
   },
   "outputs": [],
   "source": "# Cell 6: Strip checkpoints + zip + save to Drive\n# ONLY RUN AFTER REVIEWING KILL TEST RESULTS\nimport os, torch, glob, shutil\nos.chdir(\"/content/competition_package\")\nos.makedirs('logs/slim', exist_ok=True)\n\nfor pt in sorted(glob.glob('logs/transformer_v1_*.pt')):\n    if '_epoch' in pt:\n        continue  # Skip periodic checkpoints\n    ckpt = torch.load(pt, map_location='cpu', weights_only=False)\n    slim = {\n        'model_state_dict': ckpt['model_state_dict'],\n        'config': ckpt.get('config', {}),\n        'best_score': ckpt.get('best_score', None),\n    }\n    out = f'logs/slim/{os.path.basename(pt)}'\n    torch.save(slim, out)\n    orig = os.path.getsize(pt) / 1e6\n    new = os.path.getsize(out) / 1e6\n    print(f'{os.path.basename(pt)}: {orig:.1f}MB -> {new:.1f}MB')\n\n# Copy normalizers\nfor npz in sorted(glob.glob('logs/normalizer_transformer_v1*.npz')):\n    shutil.copy(npz, f'logs/slim/{os.path.basename(npz)}')\n    print(f'Copied {os.path.basename(npz)}')\n\nprint(f'\\n--- logs/slim/ contents ({len(os.listdir(\"logs/slim\"))} files) ---')\ntotal_mb = 0\nfor f in sorted(os.listdir('logs/slim')):\n    sz = os.path.getsize(f'logs/slim/{f}') / 1e6\n    total_mb += sz\n    print(f'  {f}: {sz:.1f}MB')\nprint(f'  Total: {total_mb:.1f}MB')\n\n# Zip for download\nshutil.make_archive('/content/transformer_kill_test', 'zip',\n                    '/content/competition_package/logs/slim')\nsz = os.path.getsize('/content/transformer_kill_test.zip') / 1e6\nprint(f'\\ntransformer_kill_test.zip: {sz:.1f}MB')\n\n# Save to Drive\nshutil.copy('/content/transformer_kill_test.zip',\n            '/content/drive/MyDrive/wunderfund/transformer_kill_test.zip')\nprint('Saved to Drive: MyDrive/wunderfund/transformer_kill_test.zip')"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}