{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Cell 0: Mount Drive, download data from Kaggle\n",
    "import os, json\n",
    "\n",
    "# Mount Drive for saving outputs\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.makedirs('/content/drive/MyDrive/wunderfund', exist_ok=True)\n",
    "\n",
    "# Install pinned kaggle + set credentials\n",
    "!pip install -q kaggle==1.6.14 --force-reinstall\n",
    "os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "    json.dump({\"username\": \"vincentvdo6\", \"key\": \"KGAT_17c43012d9e77edf2c183a25acb1489b\"}, f)\n",
    "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
    "\n",
    "# Download + unzip dataset\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "!kaggle datasets download -d vincentvdo6/wunderfund-predictorium -p /content/data/ --force\n",
    "!unzip -o -q /content/data/wunderfund-predictorium.zip -d /content/data/\n",
    "!ls /content/data/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Setup — clone repo, link data\n",
    "import os, subprocess\n",
    "REPO = \"/content/competition_package\"\n",
    "\n",
    "os.chdir(\"/content\")\n",
    "os.system(f\"rm -rf {REPO}\")\n",
    "os.system(f\"git clone https://github.com/vincentvdo6/competition_package.git {REPO}\")\n",
    "os.chdir(REPO)\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Link data from Kaggle download\n",
    "os.system('ln -sf /content/data/train.parquet datasets/train.parquet')\n",
    "os.system('ln -sf /content/data/valid.parquet datasets/valid.parquet')\n",
    "\n",
    "# Verify\n",
    "assert os.path.exists(\"datasets/train.parquet\"), \"train.parquet not found!\"\n",
    "assert os.path.exists(\"datasets/valid.parquet\"), \"valid.parquet not found!\"\n",
    "print(\"Commit:\", subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"], text=True).strip())\n",
    "print(f\"GPU: {os.popen('nvidia-smi --query-gpu=name --format=csv,noheader').read().strip()}\")\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-treatment"
   },
   "outputs": [],
   "source": "# Cell 2: Treatment — aux heads, 3 seeds\n# Expected: ~5-7 min per seed (35 epochs max, early stopping)\nimport os, subprocess\nos.chdir(\"/content/competition_package\")\n\nfor seed in [42, 43, 44]:\n    print(f\"\\n{'='*60}\")\n    print(f'Training gru_aux_heads_v1 seed {seed}')\n    print(f\"{'='*60}\", flush=True)\n    p = subprocess.Popen(\n        ['python', '-u', 'scripts/train.py',\n         '--config', 'configs/gru_aux_heads_v1.yaml',\n         '--seed', str(seed), '--device', 'cuda'],\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n        text=True, bufsize=1\n    )\n    for line in p.stdout:\n        print(line, end='')\n    rc = p.wait()\n    if rc != 0:\n        print(f'ERROR: seed {seed} failed with exit code {rc}')\n\nprint('\\nTreatment training done!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-control"
   },
   "outputs": [],
   "source": "# Cell 3: Control — pearson_combined loss (current p1), 3 seeds\n# Same seeds for direct comparison\nimport os, subprocess\nos.chdir(\"/content/competition_package\")\n\nfor seed in [42, 43, 44]:\n    print(f\"\\n{'='*60}\")\n    print(f'Training gru_pearson_v1 (control) seed {seed}')\n    print(f\"{'='*60}\", flush=True)\n    p = subprocess.Popen(\n        ['python', '-u', 'scripts/train.py',\n         '--config', 'configs/gru_pearson_v1.yaml',\n         '--seed', str(seed), '--device', 'cuda'],\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n        text=True, bufsize=1\n    )\n    for line in p.stdout:\n        print(line, end='')\n    rc = p.wait()\n    if rc != 0:\n        print(f'ERROR: seed {seed} failed with exit code {rc}')\n\nprint('\\nControl training done!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": "# Cell 4: Evaluate all 6 models — treatment vs control\nimport os, torch, glob\nos.chdir(\"/content/competition_package\")\n\ntreatment_scores = []\ncontrol_scores = []\n\nprint('--- Treatment (aux heads) ---')\nfor pt in sorted(glob.glob('logs/gru_aux_heads_v1_seed*.pt')):\n    if '_epoch' in pt:\n        continue  # Skip periodic checkpoints\n    ckpt = torch.load(pt, map_location='cpu', weights_only=False)\n    score = float(ckpt.get('best_score', 0.0))\n    epoch = ckpt.get('best_epoch', 0)\n    name = os.path.basename(pt)\n    treatment_scores.append(score)\n    print(f'  {name}: val={score:.4f} (best epoch {epoch})')\n\nprint('\\n--- Control (pearson_combined / p1) ---')\nfor pt in sorted(glob.glob('logs/gru_pearson_v1_seed*.pt')):\n    if '_epoch' in pt:\n        continue\n    ckpt = torch.load(pt, map_location='cpu', weights_only=False)\n    score = float(ckpt.get('best_score', 0.0))\n    epoch = ckpt.get('best_epoch', 0)\n    name = os.path.basename(pt)\n    control_scores.append(score)\n    print(f'  {name}: val={score:.4f} (best epoch {epoch})')\n\nif treatment_scores and control_scores:\n    t_mean = sum(treatment_scores) / len(treatment_scores)\n    c_mean = sum(control_scores) / len(control_scores)\n    deltas = [t - c for t, c in zip(treatment_scores, control_scores)]\n    mean_delta = sum(deltas) / len(deltas)\n    n_positive = sum(1 for d in deltas if d > 0)\n\n    print(f'\\n{\"=\"*60}')\n    print(f'KILL TEST RESULTS')\n    print(f'{\"=\"*60}')\n    print(f'Treatment mean: {t_mean:.4f}')\n    print(f'Control mean:   {c_mean:.4f}')\n    print(f'Mean delta:     {mean_delta:+.4f}')\n    print(f'Per-seed deltas: {[\"{:+.4f}\".format(d) for d in deltas]}')\n    print(f'Positive seeds: {n_positive}/{len(deltas)}')\n    print()\n\n    # Kill test criteria: mean delta >= +0.0010 AND >= 2/3 positive\n    pass_mean = mean_delta >= 0.0010\n    pass_count = n_positive >= 2\n    if pass_mean and pass_count:\n        print('PASS! Proceed to seed expansion.')\n    elif pass_mean:\n        print(f'MARGINAL: mean delta OK ({mean_delta:+.4f}) but only {n_positive}/3 positive.')\n    elif pass_count:\n        print(f'MARGINAL: {n_positive}/3 positive but mean delta too small ({mean_delta:+.4f}).')\n    else:\n        print(f'FAIL: mean delta {mean_delta:+.4f} < 0.0010 and {n_positive}/3 positive.')\nelse:\n    print('ERROR: Missing checkpoints! Check training output above.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-curves"
   },
   "outputs": [],
   "source": "# Cell 5: Training curves — compare treatment vs control\nimport json, glob, os\nos.chdir(\"/content/competition_package\")\n\nfor pattern, label in [\n    ('logs/training_history_gru_aux_heads_v1*.json', 'Treatment'),\n    ('logs/training_history_gru_pearson_v1*.json', 'Control'),\n]:\n    print(f'\\n--- {label} ---')\n    for hist_file in sorted(glob.glob(pattern)):\n        with open(hist_file) as f:\n            hist = json.load(f)\n        name = os.path.basename(hist_file).replace('training_history_', '').replace('.json', '')\n        scores = [s['avg'] for s in hist['val_scores']]\n        t0_scores = [s['t0'] for s in hist['val_scores']]\n        t1_scores = [s['t1'] for s in hist['val_scores']]\n        lrs = hist['learning_rates']\n        best_idx = scores.index(max(scores))\n        print(f'  {name}:')\n        print(f'    Epochs: {len(scores)}, Best avg: {max(scores):.4f} at epoch {best_idx+1}')\n        print(f'    Best t0: {t0_scores[best_idx]:.4f}, Best t1: {t1_scores[best_idx]:.4f}')\n        print(f'    t0/t1 ratio: {t0_scores[best_idx]/max(t1_scores[best_idx], 1e-8):.2f}')\n        print(f'    Last 5 avg: {[\"{:.4f}\".format(s) for s in scores[-5:]]}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "strip-zip"
   },
   "outputs": [],
   "source": "# Cell 6: Strip checkpoints + zip + save to Drive\n# ONLY RUN AFTER REVIEWING KILL TEST RESULTS\nimport os, torch, glob, shutil\nos.chdir(\"/content/competition_package\")\nos.makedirs('logs/slim', exist_ok=True)\n\npatterns = (\n    glob.glob('logs/gru_aux_heads_v1_*.pt') +\n    glob.glob('logs/gru_pearson_v1_*.pt')\n)\n\nfor pt in sorted(patterns):\n    if '_epoch' in pt:\n        continue  # Skip periodic checkpoints\n    ckpt = torch.load(pt, map_location='cpu', weights_only=False)\n    slim = {\n        'model_state_dict': ckpt['model_state_dict'],\n        'config': ckpt.get('config', {}),\n        'best_score': ckpt.get('best_score', None),\n    }\n    out = f'logs/slim/{os.path.basename(pt)}'\n    torch.save(slim, out)\n    orig = os.path.getsize(pt) / 1e6\n    new = os.path.getsize(out) / 1e6\n    print(f'{os.path.basename(pt)}: {orig:.1f}MB -> {new:.1f}MB')\n\n# Copy normalizers\nfor npz in sorted(\n    glob.glob('logs/normalizer_gru_aux_heads*.npz') +\n    glob.glob('logs/normalizer_gru_pearson_v1*.npz')\n):\n    shutil.copy(npz, f'logs/slim/{os.path.basename(npz)}')\n    print(f'Copied {os.path.basename(npz)}')\n\nprint(f'\\n--- logs/slim/ contents ({len(os.listdir(\"logs/slim\"))} files) ---')\ntotal_mb = 0\nfor f in sorted(os.listdir('logs/slim')):\n    sz = os.path.getsize(f'logs/slim/{f}') / 1e6\n    total_mb += sz\n    print(f'  {f}: {sz:.1f}MB')\nprint(f'  Total: {total_mb:.1f}MB')\n\n# Zip for download\nshutil.make_archive('/content/aux_heads_kill_test', 'zip',\n                    '/content/competition_package/logs/slim')\nsz = os.path.getsize('/content/aux_heads_kill_test.zip') / 1e6\nprint(f'\\naux_heads_kill_test.zip: {sz:.1f}MB')\n\n# Save to Drive\nshutil.copy('/content/aux_heads_kill_test.zip',\n            '/content/drive/MyDrive/wunderfund/aux_heads_kill_test.zip')\nprint('Saved to Drive: MyDrive/wunderfund/aux_heads_kill_test.zip')"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}