config,timestamp,model,val_score_t0,val_score_t1,val_score_avg,notes,best_score,best_epoch,epochs_trained,final_lr,param_count_from_notes,model_type,input_size,hidden_size,num_layers,dropout,lr,weight_decay,batch_size,weighted_ratio,target_weights,early_stopping_patience,derived_features,scheduler_type
configs/gru_derived_v1.yaml,2026-02-06T01:00:00,gru,0.3912,0.1316,0.2614,GRU + derived features (42 input). First run. Best so far.,0.2614,7.0,17.0,,,gru,42,128,2,0.2,0.001,1e-05,256,0.6,None,10,True,reduce_on_plateau
configs/gru_long_memory_derived_v1.yaml,2026-02-06T02:30:00,gru,0.3869,0.1348,0.2609,GRU 192h/3L + derived (694K params). Overfits fast (best epoch 4). Bigger model not better.,0.2609,4.0,14.0,,694000.0,gru,42,192,3,0.15,0.0008,2e-05,192,0.65,"[0.4, 0.6]",12,True,reduce_on_plateau
configs/gru_baseline.yaml,2026-02-05T12:00:00,gru,0.3869,0.1286,0.2578,"GRU baseline trained on Kaggle Tesla T4. 2-layer GRU, hidden=128, dropout=0.2, batch=256, CombinedLoss. Early stopped at epoch 18. Total time: 62s.",0.2578,8.0,18.0,0.000125,,gru,32,128,2,0.2,0.001,1e-05,256,0.5,None,10,False,reduce_on_plateau
configs/lstm_derived_v1.yaml,2026-02-06T03:00:00,lstm,0.3836,0.1247,0.2542,LSTM 128h/2L + derived (278K params). Underperforms GRU. Cell state doesn't help for LOB data.,0.2542,11.0,,,278000.0,lstm,42,128,2,0.2,0.001,1e-05,256,0.6,"[0.4, 0.6]",10,True,reduce_on_plateau
configs/gru_derived_t1focus_v1.yaml,2026-02-06T02:00:00,gru,0.3854,0.1218,0.2536,"GRU + derived + target_weights [0.35, 0.65]. Worse than equal weights â€” t1 emphasis hurts both targets.",0.2536,9.0,,,,gru,42,128,2,0.2,0.001,1e-05,256,0.6,"[0.4, 0.6]",10,True,reduce_on_plateau
