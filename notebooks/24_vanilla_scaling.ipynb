{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla GRU Scaling\n",
    "**Breakthrough**: Vanilla GRU (h=64, 3L, raw32, no norm, MSE, linear output) scored **0.2814 LB** — new PB!\n",
    "Gap flipped **POSITIVE** (+0.0077) vs tightwd_v2's -0.0004. Only 0.0020 from top 100.\n",
    "\n",
    "**Scaling test**: Train h=128, h=144, h=192 (3L), h=192 (2L) with 3 seeds each.\n",
    "Pick best config → seed expansion → single-model LB test → ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Mount Drive, download data from Kaggle\n",
    "import os, json\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.makedirs('/content/drive/MyDrive/wunderfund', exist_ok=True)\n",
    "\n",
    "!pip install -q kaggle==1.6.14 --force-reinstall\n",
    "os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "    json.dump({\"username\": \"vincentvdo6\", \"key\": \"FILL_IN\"}, f)\n",
    "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
    "\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "!kaggle datasets download -d vincentvdo6/wunderfund-predictorium -p /content/data/ --force\n",
    "!unzip -o -q /content/data/wunderfund-predictorium.zip -d /content/data/\n",
    "!ls /content/data/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup — clone repo, link data\n",
    "import os, subprocess\n",
    "REPO = \"/content/competition_package\"\n",
    "\n",
    "os.chdir(\"/content\")\n",
    "subprocess.run([\"rm\", \"-rf\", REPO], check=False)\n",
    "subprocess.run([\"git\", \"clone\", \"https://github.com/vincentvdo6/competition_package.git\", REPO], check=True)\n",
    "os.chdir(REPO)\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "subprocess.run([\"ln\", \"-sf\", \"/content/data/train.parquet\", \"datasets/train.parquet\"], check=True)\n",
    "subprocess.run([\"ln\", \"-sf\", \"/content/data/valid.parquet\", \"datasets/valid.parquet\"], check=True)\n",
    "\n",
    "assert os.path.exists(\"datasets/train.parquet\"), \"train.parquet not found!\"\n",
    "assert os.path.exists(\"datasets/valid.parquet\"), \"valid.parquet not found!\"\n",
    "commit = subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"], text=True).strip()\n",
    "print(f\"Commit: {commit}\")\n",
    "print(f\"GPU: {subprocess.check_output(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], text=True).strip()}\")\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Train ALL vanilla scaling configs (h=128, h=144, h=192 3L, h=192 2L) x 3 seeds\n",
    "import os, subprocess, sys\n",
    "os.chdir(\"/content/competition_package\")\n",
    "\n",
    "CONFIGS = [\n",
    "    (\"vanilla_h128\", \"configs/vanilla_h128.yaml\"),\n",
    "    (\"vanilla_h192\", \"configs/vanilla_h192.yaml\"),\n",
    "    (\"vanilla_h144\", \"configs/vanilla_h144.yaml\"),\n",
    "    (\"vanilla_h192_2L\", \"configs/vanilla_h192_2L.yaml\"),\n",
    "]\n",
    "SEEDS = [42, 43, 44]\n",
    "\n",
    "print(f\"=== VANILLA GRU SCALING ===\")\n",
    "print(f\"Configs: {len(CONFIGS)} x {len(SEEDS)} seeds = {len(CONFIGS)*len(SEEDS)} runs\")\n",
    "print(\"=\" * 60, flush=True)\n",
    "\n",
    "for config_name, config_path in CONFIGS:\n",
    "    for seed in SEEDS:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training {config_name} seed {seed}\")\n",
    "        print(f\"{'='*60}\", flush=True)\n",
    "        proc = subprocess.Popen(\n",
    "            [sys.executable, \"-u\", \"scripts/train.py\",\n",
    "             \"--config\", config_path,\n",
    "             \"--seed\", str(seed), \"--device\", \"cuda\"],\n",
    "            stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
    "        )\n",
    "        for line in proc.stdout:\n",
    "            print(line, end=\"\", flush=True)\n",
    "        proc.wait()\n",
    "        if proc.returncode != 0:\n",
    "            print(f\"ERROR: {config_name} seed {seed} failed with return code {proc.returncode}\")\n",
    "\n",
    "print(f\"\\nAll training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Evaluation — compare all configs\n",
    "import os, glob, torch\n",
    "os.chdir(\"/content/competition_package\")\n",
    "\n",
    "# Reference points\n",
    "PARITY_V1_VAL = 0.2692   # mean val (3 seeds)\n",
    "PARITY_V1_LB = 0.2814    # best seed (s43, val 0.2737)\n",
    "TIGHTWD_VAL = 0.2660     # our previous best single-model recipe\n",
    "OFFICIAL_LB = 0.2761     # official baseline h=64\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for config_name in [\"vanilla_h128\", \"vanilla_h144\", \"vanilla_h192\", \"vanilla_h192_2L\"]:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  {config_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = []\n",
    "    for pt in sorted(glob.glob(f\"logs/{config_name}_seed*.pt\")):\n",
    "        basename = os.path.basename(pt)\n",
    "        if '_epoch' in basename:\n",
    "            continue\n",
    "        ckpt = torch.load(pt, map_location=\"cpu\", weights_only=False)\n",
    "        score = float(ckpt.get(\"best_score\", 0))\n",
    "        epoch = ckpt.get(\"best_epoch\", \"N/A\")\n",
    "        results.append((basename, score, epoch))\n",
    "    \n",
    "    if not results:\n",
    "        print(\"  No checkpoints found!\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"{'Model':<50} {'Val Score':>10} {'Epoch':>6}\")\n",
    "    print(\"-\" * 70)\n",
    "    for name, score, epoch in results:\n",
    "        print(f\"{name:<50} {score:>10.4f} {str(epoch):>6}\")\n",
    "    \n",
    "    scores = [s for _, s, _ in results]\n",
    "    mean_val = sum(scores) / len(scores)\n",
    "    best_val = max(scores)\n",
    "    best_name = [n for n, s, _ in results if s == best_val][0]\n",
    "    \n",
    "    all_results[config_name] = {\n",
    "        'mean': mean_val, 'best': best_val, 'best_name': best_name,\n",
    "        'scores': scores, 'results': results\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nMean val: {mean_val:.4f} (vs parity_v1 h=64: {mean_val - PARITY_V1_VAL:+.4f})\")\n",
    "    print(f\"Best val: {best_val:.4f} ({best_name})\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Config':<25} {'Mean Val':>10} {'Best Val':>10} {'vs h=64':>10} {'Best Seed':>30}\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'parity_v1 (h=64, 3L)':<25} {PARITY_V1_VAL:>10.4f} {'0.2737':>10} {'baseline':>10} {'s43':>30}\")\n",
    "for name, data in sorted(all_results.items(), key=lambda x: -x[1]['mean']):\n",
    "    delta = data['mean'] - PARITY_V1_VAL\n",
    "    print(f\"{name:<25} {data['mean']:>10.4f} {data['best']:>10.4f} {delta:>+10.4f} {data['best_name']:>30}\")\n",
    "\n",
    "print(f\"\\nINTERPRETATION:\")\n",
    "print(f\"parity_v1 h=64: val=0.2692, LB=0.2814 (gap +0.0122)\")\n",
    "print(f\"If mean val >= 0.275: STRONG PASS — submit best seed immediately\")\n",
    "print(f\"If mean val 0.270-0.275: PASS — submit for LB gap test\")\n",
    "print(f\"If mean val < 0.265: FAIL — larger vanilla GRU may overfit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Strip checkpoints + zip + save to Drive\n",
    "import os, torch, glob, shutil\n",
    "os.chdir(\"/content/competition_package\")\n",
    "os.makedirs(\"logs/slim\", exist_ok=True)\n",
    "\n",
    "for pattern in [\"vanilla_h128\", \"vanilla_h144\", \"vanilla_h192\", \"vanilla_h192_2L\"]:\n",
    "    for pt in sorted(glob.glob(f\"logs/{pattern}*.pt\")):\n",
    "        basename = os.path.basename(pt)\n",
    "        if '_epoch' in basename:\n",
    "            continue\n",
    "        ckpt = torch.load(pt, map_location=\"cpu\", weights_only=False)\n",
    "        slim = {\n",
    "            \"model_state_dict\": ckpt[\"model_state_dict\"],\n",
    "            \"config\": ckpt.get(\"config\", {}),\n",
    "            \"best_score\": ckpt.get(\"best_score\", None),\n",
    "            \"best_epoch\": ckpt.get(\"best_epoch\", None),\n",
    "        }\n",
    "        out = f\"logs/slim/{basename}\"\n",
    "        torch.save(slim, out)\n",
    "        orig = os.path.getsize(pt) / 1e6\n",
    "        new = os.path.getsize(out) / 1e6\n",
    "        print(f\"{basename}: {orig:.1f}MB -> {new:.1f}MB\")\n",
    "\n",
    "shutil.make_archive(\"/content/vanilla_scaling\", \"zip\",\n",
    "                     \"/content/competition_package/logs/slim\")\n",
    "sz = os.path.getsize(\"/content/vanilla_scaling.zip\") / 1e6\n",
    "print(f\"\\nvanilla_scaling.zip: {sz:.1f}MB\")\n",
    "\n",
    "shutil.copy(\"/content/vanilla_scaling.zip\",\n",
    "            \"/content/drive/MyDrive/wunderfund/vanilla_scaling.zip\")\n",
    "print(\"Saved to Drive: MyDrive/wunderfund/vanilla_scaling.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
