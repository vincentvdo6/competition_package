{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Seed Expansion — pearson_v1 seeds 51-70 (Colab)\n",
    "Trains 20 new pearson_v1 GRU seeds on Colab GPU.\n",
    "Run the tightwd_v2 seeds (54-73) on Kaggle in parallel."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 0: Mount Drive, download data from Kaggle\nimport os, json\n\n# Mount Drive for saving outputs\nfrom google.colab import drive\ndrive.mount('/content/drive')\nos.makedirs('/content/drive/MyDrive/wunderfund', exist_ok=True)\n\n# Install pinned kaggle + set credentials\n!pip install -q kaggle==1.6.14 --force-reinstall\nos.makedirs('/root/.kaggle', exist_ok=True)\nwith open('/root/.kaggle/kaggle.json', 'w') as f:\n    json.dump({\"username\": \"vincentvdo6\", \"key\": \"KGAT_17c43012d9e77edf2c183a25acb1489b\"}, f)\nos.chmod('/root/.kaggle/kaggle.json', 0o600)\n\n# Download + unzip dataset\nos.makedirs('/content/data', exist_ok=True)\n!kaggle datasets download -d vincentvdo6/wunderfund-predictorium -p /content/data/ --force\n!unzip -o -q /content/data/wunderfund-predictorium.zip -d /content/data/\n!ls /content/data/*.parquet",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup — clone repo, link data\n",
    "import os, subprocess\n",
    "REPO = \"/content/competition_package\"\n",
    "\n",
    "os.chdir(\"/content\")\n",
    "os.system(f\"rm -rf {REPO}\")\n",
    "os.system(f\"git clone https://github.com/vincentvdo6/competition_package.git {REPO}\")\n",
    "os.chdir(REPO)\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Link data from Kaggle download\n",
    "os.system('ln -sf /content/data/train.parquet datasets/train.parquet')\n",
    "os.system('ln -sf /content/data/valid.parquet datasets/valid.parquet')\n",
    "\n",
    "# Verify\n",
    "assert os.path.exists(\"datasets/train.parquet\"), \"train.parquet not found!\"\n",
    "assert os.path.exists(\"datasets/valid.parquet\"), \"valid.parquet not found!\"\n",
    "print(\"Commit:\", subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"], text=True).strip())\n",
    "print(f\"GPU: {os.popen('nvidia-smi --query-gpu=name --format=csv,noheader').read().strip()}\")\n",
    "print(\"Ready!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: pearson_v1 seeds 51-60 (10 seeds, ~16 min on T4)\n",
    "# Existing p1: 42-50 (9 seeds). This adds 10 more.\n",
    "import os\n",
    "os.chdir(\"/content/competition_package\")\n",
    "for seed in range(51, 61):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training gru_pearson_v1 seed {seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    os.system(\n",
    "        f\"python -u scripts/train.py \"\n",
    "        f\"--config configs/gru_pearson_v1.yaml \"\n",
    "        f\"--seed {seed} --device cuda\"\n",
    "    )\n",
    "print(\"\\nBatch 1 done: p1 seeds 51-60\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: pearson_v1 seeds 61-70 (10 seeds, ~16 min on T4)\n",
    "import os\n",
    "os.chdir(\"/content/competition_package\")\n",
    "for seed in range(61, 71):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training gru_pearson_v1 seed {seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    os.system(\n",
    "        f\"python -u scripts/train.py \"\n",
    "        f\"--config configs/gru_pearson_v1.yaml \"\n",
    "        f\"--seed {seed} --device cuda\"\n",
    "    )\n",
    "print(\"\\nBatch 2 done: p1 seeds 61-70\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Strip checkpoints + copy normalizers + zip + save to Drive\n",
    "import os, torch, glob, shutil\n",
    "os.chdir(\"/content/competition_package\")\n",
    "os.makedirs(\"logs/slim\", exist_ok=True)\n",
    "\n",
    "for pt in sorted(glob.glob(\"logs/*.pt\")):\n",
    "    try:\n",
    "        ckpt = torch.load(pt, map_location=\"cpu\", weights_only=False)\n",
    "    except TypeError:\n",
    "        ckpt = torch.load(pt, map_location=\"cpu\")\n",
    "    slim = {\n",
    "        \"model_state_dict\": ckpt[\"model_state_dict\"],\n",
    "        \"config\": ckpt.get(\"config\", {}),\n",
    "        \"best_score\": ckpt.get(\"best_score\", None),\n",
    "    }\n",
    "    out = f\"logs/slim/{os.path.basename(pt)}\"\n",
    "    torch.save(slim, out)\n",
    "    orig = os.path.getsize(pt) / 1e6\n",
    "    new = os.path.getsize(out) / 1e6\n",
    "    print(f\"{os.path.basename(pt)}: {orig:.1f}MB -> {new:.1f}MB\")\n",
    "\n",
    "for npz in sorted(glob.glob(\"logs/normalizer_*.npz\")):\n",
    "    shutil.copy(npz, f\"logs/slim/{os.path.basename(npz)}\")\n",
    "    print(f\"Copied {os.path.basename(npz)}\")\n",
    "\n",
    "print(f\"\\n--- logs/slim/ contents ({len(os.listdir('logs/slim'))} files) ---\")\n",
    "for f in sorted(os.listdir(\"logs/slim\")):\n",
    "    sz = os.path.getsize(f\"logs/slim/{f}\") / 1e6\n",
    "    print(f\"  {f}: {sz:.1f}MB\")\n",
    "\n",
    "# Zip for download\n",
    "shutil.make_archive(\"/content/p1_expansion\", \"zip\",\n",
    "                     \"/content/competition_package/logs/slim\")\n",
    "sz = os.path.getsize(\"/content/p1_expansion.zip\") / 1e6\n",
    "print(f\"\\np1_expansion.zip: {sz:.1f}MB\")\n",
    "\n",
    "# Save to Drive for persistence\n",
    "shutil.copy(\"/content/p1_expansion.zip\", \"/content/drive/MyDrive/wunderfund/p1_expansion.zip\")\n",
    "print(\"Saved to Drive: MyDrive/wunderfund/p1_expansion.zip\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Print validation scores (sorted best to worst)\n",
    "import os, glob, torch\n",
    "os.chdir(\"/content/competition_package\")\n",
    "\n",
    "results = []\n",
    "for pt in sorted(glob.glob(\"logs/*.pt\")):\n",
    "    try:\n",
    "        ckpt = torch.load(pt, map_location=\"cpu\", weights_only=False)\n",
    "    except TypeError:\n",
    "        ckpt = torch.load(pt, map_location=\"cpu\")\n",
    "    score = ckpt.get(\"best_score\", ckpt.get(\"val_score\", None))\n",
    "    name = os.path.basename(pt)\n",
    "    if isinstance(score, (int, float)):\n",
    "        results.append((name, float(score)))\n",
    "    else:\n",
    "        results.append((name, 0.0))\n",
    "\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"{'Rank':<5} {'Model':<55} {'Val Score':>10}\")\n",
    "print(\"-\" * 72)\n",
    "for i, (name, score) in enumerate(results, 1):\n",
    "    marker = \" *\" if score >= 0.264 else \"\"\n",
    "    print(f\"{i:<5} {name:<55} {score:>10.4f}{marker}\")\n",
    "\n",
    "scores = [s for _, s in results]\n",
    "print(f\"\\n--- pearson_v1: {len(results)} seeds ---\")\n",
    "print(f\"  Best 5: {sorted(scores, reverse=True)[:5]}\")\n",
    "print(f\"  Mean: {sum(scores)/len(scores):.4f}\")\n",
    "print(f\"  Std: {(sum((s-sum(scores)/len(scores))**2 for s in scores)/len(scores))**0.5:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}