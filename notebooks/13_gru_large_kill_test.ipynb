{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 0: Mount Drive, download data from Kaggle\nimport os, json\n\n# Mount Drive for saving outputs\nfrom google.colab import drive\ndrive.mount('/content/drive')\nos.makedirs('/content/drive/MyDrive/wunderfund', exist_ok=True)\n\n# Install pinned kaggle + set credentials\n!pip install -q kaggle==1.6.14 --force-reinstall\nos.makedirs('/root/.kaggle', exist_ok=True)\nwith open('/root/.kaggle/kaggle.json', 'w') as f:\n    json.dump({\"username\": \"vincentvdo6\", \"key\": \"KGAT_17c43012d9e77edf2c183a25acb1489b\"}, f)\nos.chmod('/root/.kaggle/kaggle.json', 0o600)\n\n# Download + unzip dataset\nos.makedirs('/content/data', exist_ok=True)\n!kaggle datasets download -d vincentvdo6/wunderfund-predictorium -p /content/data/ --force\n!unzip -o -q /content/data/wunderfund-predictorium.zip -d /content/data/\n!ls /content/data/*.parquet"
  },
  {
   "cell_type": "code",
   "id": "sz9zzhgoulj",
   "source": "# Cell 1: Setup — clone repo, link data\nimport os, subprocess\nREPO = \"/content/competition_package\"\n\nos.chdir(\"/content\")\nos.system(f\"rm -rf {REPO}\")\nos.system(f\"git clone https://github.com/vincentvdo6/competition_package.git {REPO}\")\nos.chdir(REPO)\nos.makedirs(\"datasets\", exist_ok=True)\nos.makedirs(\"logs\", exist_ok=True)\n\n# Link data from Kaggle download\nos.system('ln -sf /content/data/train.parquet datasets/train.parquet')\nos.system('ln -sf /content/data/valid.parquet datasets/valid.parquet')\n\n# Verify\nassert os.path.exists(\"datasets/train.parquet\"), \"train.parquet not found!\"\nassert os.path.exists(\"datasets/valid.parquet\"), \"valid.parquet not found!\"\nprint(\"Commit:\", subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"], text=True).strip())\nprint(f\"GPU: {os.popen('nvidia-smi --query-gpu=name --format=csv,noheader').read().strip()}\")\nprint(\"Ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kill-test-h256",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Kill Test — gru_large_v1 (h=256) seeds 42-43\n# Expected: ~8-10 min per seed (60 epochs max, early stopping ~30-40)\n# PASS threshold: mean val > 0.270 (current GRU mean is 0.262-0.263)\nimport os\nos.chdir(\"/content/competition_package\")\n\nfor seed in [42, 43]:\n    print(f\"\\n{'='*60}\")\n    print(f'Training gru_large_v1 (h=256) seed {seed}')\n    print(f\"{'='*60}\")\n    os.system(\n        f'python -u scripts/train.py '\n        f'--config configs/gru_large_v1.yaml '\n        f'--seed {seed} --device cuda'\n    )\n\nprint('\\nKill test h=256 done!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-h256",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Evaluate kill test results\nimport os, torch, glob\nos.chdir(\"/content/competition_package\")\n\nresults = []\nfor pt in sorted(glob.glob('logs/gru_large_v1_seed*.pt')):\n    ckpt = torch.load(pt, map_location='cpu', weights_only=False)\n    score = ckpt.get('best_score', 0.0)\n    epoch = ckpt.get('best_epoch', 0)\n    name = os.path.basename(pt)\n    results.append((name, float(score), epoch))\n    print(f'{name}: val={score:.4f} (best epoch {epoch})')\n\nif results:\n    scores = [s for _, s, _ in results]\n    mean_score = sum(scores) / len(scores)\n    print(f'\\n--- Kill Test Result ---')\n    print(f'Mean val: {mean_score:.4f}')\n    print(f'Current GRU baseline mean: ~0.2620')\n    print(f'Delta: {mean_score - 0.2620:+.4f}')\n    if mean_score > 0.275:\n        print('STRONG PASS! Proceed to 5-seed expansion.')\n    elif mean_score > 0.270:\n        print('PASS! Proceed to 5-seed expansion.')\n    elif mean_score > 0.265:\n        print('MARGINAL. Consider testing h=256 with old recipe to isolate effect.')\n    else:\n        print('FAIL. Try h=256 with current recipe (no cosine/EMA) to isolate.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expansion-h256",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Full expansion — gru_large_v1 seeds 44-48 (5 more seeds)\n# ONLY RUN IF KILL TEST PASSED (mean > 0.270)\nimport os\nos.chdir(\"/content/competition_package\")\n\nfor seed in range(44, 49):\n    print(f\"\\n{'='*60}\")\n    print(f'Training gru_large_v1 (h=256) seed {seed}')\n    print(f\"{'='*60}\")\n    os.system(\n        f'python -u scripts/train.py '\n        f'--config configs/gru_large_v1.yaml '\n        f'--seed {seed} --device cuda'\n    )\n\nprint('\\nExpansion done: h=256 seeds 44-48')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kill-test-h384",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Kill Test — gru_xl_v1 (h=384) seeds 42-43\n# ONLY RUN IF h=256 PASSED and you have time remaining\n# Expected: ~15-20 min per seed\nimport os\nos.chdir(\"/content/competition_package\")\n\nfor seed in [42, 43]:\n    print(f\"\\n{'='*60}\")\n    print(f'Training gru_xl_v1 (h=384) seed {seed}')\n    print(f\"{'='*60}\")\n    os.system(\n        f'python -u scripts/train.py '\n        f'--config configs/gru_xl_v1.yaml '\n        f'--seed {seed} --device cuda'\n    )\n\nprint('\\nKill test h=384 done!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Evaluate all results (h=256 + h=384)\n",
    "import os, torch, glob\n",
    "os.chdir(REPO)\n",
    "\n",
    "for config_name in ['gru_large_v1', 'gru_xl_v1']:\n",
    "    pts = sorted(glob.glob(f'logs/{config_name}_seed*.pt'))\n",
    "    if not pts:\n",
    "        continue\n",
    "    print(f'\\n--- {config_name} ---')\n",
    "    scores = []\n",
    "    for pt in pts:\n",
    "        ckpt = torch.load(pt, map_location='cpu', weights_only=False)\n",
    "        score = float(ckpt.get('best_score', 0.0))\n",
    "        epoch = ckpt.get('best_epoch', 0)\n",
    "        name = os.path.basename(pt)\n",
    "        scores.append(score)\n",
    "        print(f'  {name}: val={score:.4f} (epoch {epoch})')\n",
    "    print(f'  Mean: {sum(scores)/len(scores):.4f}, Best: {max(scores):.4f}, Worst: {min(scores):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strip-zip",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Strip checkpoints + copy normalizers + zip + save to Drive\nimport os, torch, glob, shutil\nos.chdir(\"/content/competition_package\")\nos.makedirs('logs/slim', exist_ok=True)\n\nfor pt in sorted(glob.glob('logs/gru_large_v1_*.pt') + glob.glob('logs/gru_xl_v1_*.pt')):\n    if '_epoch' in pt:\n        continue  # Skip periodic checkpoints, only keep best\n    ckpt = torch.load(pt, map_location='cpu', weights_only=False)\n    slim = {\n        'model_state_dict': ckpt['model_state_dict'],\n        'config': ckpt.get('config', {}),\n        'best_score': ckpt.get('best_score', None),\n    }\n    out = f'logs/slim/{os.path.basename(pt)}'\n    torch.save(slim, out)\n    orig = os.path.getsize(pt) / 1e6\n    new = os.path.getsize(out) / 1e6\n    print(f'{os.path.basename(pt)}: {orig:.1f}MB -> {new:.1f}MB')\n\nfor npz in sorted(glob.glob('logs/normalizer_gru_large*.npz') + glob.glob('logs/normalizer_gru_xl*.npz')):\n    shutil.copy(npz, f'logs/slim/{os.path.basename(npz)}')\n    print(f'Copied {os.path.basename(npz)}')\n\nprint(f'\\n--- logs/slim/ contents ({len(os.listdir(\"logs/slim\"))} files) ---')\ntotal_mb = 0\nfor f in sorted(os.listdir('logs/slim')):\n    sz = os.path.getsize(f'logs/slim/{f}') / 1e6\n    total_mb += sz\n    print(f'  {f}: {sz:.1f}MB')\nprint(f'  Total: {total_mb:.1f}MB')\n\n# Zip for download\nshutil.make_archive('/content/gru_large_kill_test', 'zip', '/content/competition_package/logs/slim')\nsz = os.path.getsize('/content/gru_large_kill_test.zip') / 1e6\nprint(f'\\ngru_large_kill_test.zip: {sz:.1f}MB')\n\n# Save to Drive for persistence\nshutil.copy('/content/gru_large_kill_test.zip', '/content/drive/MyDrive/wunderfund/gru_large_kill_test.zip')\nprint('Saved to Drive: MyDrive/wunderfund/gru_large_kill_test.zip')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-curves",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Training curve comparison (cosine vs plateau)\nimport json, glob, os\nos.chdir(\"/content/competition_package\")\n\nfor hist_file in sorted(glob.glob('logs/training_history_gru_large*.json') + glob.glob('logs/training_history_gru_xl*.json')):\n    with open(hist_file) as f:\n        hist = json.load(f)\n    name = os.path.basename(hist_file).replace('training_history_', '').replace('.json', '')\n    scores = [s['avg'] for s in hist['val_scores']]\n    lrs = hist['learning_rates']\n    print(f'\\n--- {name} ---')\n    print(f'  Epochs: {len(scores)}, Best: {max(scores):.4f} at epoch {scores.index(max(scores))+1}')\n    print(f'  LR range: {min(lrs):.2e} to {max(lrs):.2e}')\n    print(f'  Last 5 val scores: {[\"{:.4f}\".format(s) for s in scores[-5:]]}')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}