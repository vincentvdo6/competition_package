{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 \u2014 EDA (Wunderfund Predictorium)\n",
        "\n",
        "This notebook covers:\n",
        "1. Data shape and integrity checks\n",
        "2. Feature distributions and train/valid shift\n",
        "3. Target structure and metric-weight concentration\n",
        "4. Feature-target relationships\n",
        "5. Temporal structure\n",
        "6. Cross-feature derived signals\n",
        "\n",
        "Artifacts are also saved under `notebooks/artifacts/01_eda/` for fast review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ks_2samp, spearmanr\n",
        "\n",
        "plt.style.use('default')\n",
        "\n",
        "ROOT = Path('..') if Path.cwd().name == 'notebooks' else Path('.')\n",
        "TRAIN_PATH = ROOT / 'datasets' / 'train.parquet'\n",
        "VALID_PATH = ROOT / 'datasets' / 'valid.parquet'\n",
        "ART_DIR = ROOT / 'notebooks' / 'artifacts' / '01_eda'\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FEATURE_COLS = [f'p{i}' for i in range(12)] + [f'v{i}' for i in range(12)] + [f'dp{i}' for i in range(4)] + [f'dv{i}' for i in range(4)]\n",
        "TARGET_COLS = ['t0', 't1']\n",
        "RNG = np.random.default_rng(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 \u2014 Data Shape & Basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "train = pd.read_parquet(TRAIN_PATH)\n",
        "valid = pd.read_parquet(VALID_PATH)\n",
        "\n",
        "train = train.sort_values(['seq_ix', 'step_in_seq']).reset_index(drop=True)\n",
        "valid = valid.sort_values(['seq_ix', 'step_in_seq']).reset_index(drop=True)\n",
        "\n",
        "print('Train shape:', train.shape)\n",
        "print('Valid shape:', valid.shape)\n",
        "print('Train sequences:', train['seq_ix'].nunique())\n",
        "print('Valid sequences:', valid['seq_ix'].nunique())\n",
        "\n",
        "train_steps = train.groupby('seq_ix')['step_in_seq'].count()\n",
        "valid_steps = valid.groupby('seq_ix')['step_in_seq'].count()\n",
        "print('Train steps per seq (min/max):', train_steps.min(), train_steps.max())\n",
        "print('Valid steps per seq (min/max):', valid_steps.min(), valid_steps.max())\n",
        "\n",
        "print('NaN total (train):', int(train.isna().sum().sum()))\n",
        "print('NaN total (valid):', int(valid.isna().sum().sum()))\n",
        "\n",
        "num_cols = train.select_dtypes(include=[np.number]).columns\n",
        "print('Inf total (train):', int(np.isinf(train[num_cols].to_numpy()).sum()))\n",
        "print('Inf total (valid):', int(np.isinf(valid[num_cols].to_numpy()).sum()))\n",
        "\n",
        "display(train.dtypes.to_frame('dtype').T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 \u2014 Feature Distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "train_stats = pd.DataFrame({\n",
        "    'mean': train[FEATURE_COLS].mean(),\n",
        "    'std': train[FEATURE_COLS].std(),\n",
        "    'min': train[FEATURE_COLS].min(),\n",
        "    'max': train[FEATURE_COLS].max(),\n",
        "    'skew': train[FEATURE_COLS].skew(),\n",
        "    'kurtosis': train[FEATURE_COLS].kurtosis(),\n",
        "})\n",
        "\n",
        "valid_stats = pd.DataFrame({\n",
        "    'mean': valid[FEATURE_COLS].mean(),\n",
        "    'std': valid[FEATURE_COLS].std(),\n",
        "    'min': valid[FEATURE_COLS].min(),\n",
        "    'max': valid[FEATURE_COLS].max(),\n",
        "    'skew': valid[FEATURE_COLS].skew(),\n",
        "    'kurtosis': valid[FEATURE_COLS].kurtosis(),\n",
        "})\n",
        "\n",
        "train_stats.to_csv(ART_DIR / 'feature_summary_train.csv')\n",
        "valid_stats.to_csv(ART_DIR / 'feature_summary_valid.csv')\n",
        "\n",
        "display(train_stats.round(4))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "shift_rows = []\n",
        "sample_n = 200_000\n",
        "for col in FEATURE_COLS:\n",
        "    tvals = train[col].to_numpy()\n",
        "    vvals = valid[col].to_numpy()\n",
        "    t = tvals[RNG.choice(len(tvals), size=min(sample_n, len(tvals)), replace=False)]\n",
        "    v = vvals[RNG.choice(len(vvals), size=min(sample_n, len(vvals)), replace=False)]\n",
        "    ks_stat, ks_p = ks_2samp(t, v)\n",
        "    mean_shift_z = abs(train_stats.loc[col, 'mean'] - valid_stats.loc[col, 'mean']) / (train_stats.loc[col, 'std'] + 1e-8)\n",
        "    shift_rows.append({'feature': col, 'ks_stat': ks_stat, 'ks_pvalue': ks_p, 'mean_shift_z': mean_shift_z})\n",
        "\n",
        "shift_df = pd.DataFrame(shift_rows).sort_values('ks_stat', ascending=False)\n",
        "shift_df.to_csv(ART_DIR / 'feature_shift_train_vs_valid.csv', index=False)\n",
        "\n",
        "print('Feature std max/min ratio:', float(train_stats['std'].max() / (train_stats['std'].min() + 1e-12)))\n",
        "display(shift_df.head(12).round(4))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(8, 4, figsize=(20, 30))\n",
        "axes = axes.flatten()\n",
        "hist_n = 150_000\n",
        "for i, col in enumerate(FEATURE_COLS):\n",
        "    ax = axes[i]\n",
        "    tvals = train[col].to_numpy()\n",
        "    vvals = valid[col].to_numpy()\n",
        "    ts = tvals[RNG.choice(len(tvals), size=min(hist_n, len(tvals)), replace=False)]\n",
        "    vs = vvals[RNG.choice(len(vvals), size=min(hist_n, len(vvals)), replace=False)]\n",
        "    lo = float(np.percentile(np.concatenate([ts, vs]), 0.5))\n",
        "    hi = float(np.percentile(np.concatenate([ts, vs]), 99.5))\n",
        "    bins = np.linspace(lo, hi, 60)\n",
        "    ax.hist(ts, bins=bins, density=True, alpha=0.5, label='train', color='#1f77b4')\n",
        "    ax.hist(vs, bins=bins, density=True, alpha=0.5, label='valid', color='#ff7f0e')\n",
        "    ax.set_title(col)\n",
        "    if i == 0:\n",
        "        ax.legend(fontsize=8)\n",
        "for j in range(len(FEATURE_COLS), len(axes)):\n",
        "    axes[j].axis('off')\n",
        "plt.suptitle('Feature Distributions: Train vs Valid (sampled)', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.savefig(ART_DIR / 'feature_hist_train_vs_valid.png', dpi=160)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 \u2014 Target Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "train_scored = train[train['need_prediction'].astype(bool)].copy()\n",
        "valid_scored = valid[valid['need_prediction'].astype(bool)].copy()\n",
        "\n",
        "print('Scored rows train:', len(train_scored))\n",
        "print('Scored rows valid:', len(valid_scored))\n",
        "print('t0/t1 correlation train:', float(train_scored[['t0', 't1']].corr().iloc[0, 1]))\n",
        "print('t0/t1 correlation valid:', float(valid_scored[['t0', 't1']].corr().iloc[0, 1]))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def weight_concentration(vals, quantiles=(0.01, 0.05, 0.1, 0.2)):\n",
        "    abs_vals = np.abs(vals)\n",
        "    total = abs_vals.sum()\n",
        "    out = {}\n",
        "    for q in quantiles:\n",
        "        thr = np.quantile(abs_vals, 1 - q)\n",
        "        share = abs_vals[abs_vals >= thr].sum() / (total + 1e-12)\n",
        "        out[f'top_{int(q*100)}pct_threshold'] = float(thr)\n",
        "        out[f'top_{int(q*100)}pct_share'] = float(share)\n",
        "    return out\n",
        "\n",
        "wc = {\n",
        "    'train_t0': weight_concentration(train_scored['t0'].to_numpy()),\n",
        "    'train_t1': weight_concentration(train_scored['t1'].to_numpy()),\n",
        "    'valid_t0': weight_concentration(valid_scored['t0'].to_numpy()),\n",
        "    'valid_t1': weight_concentration(valid_scored['t1'].to_numpy()),\n",
        "}\n",
        "wc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
        "for r, target in enumerate(TARGET_COLS):\n",
        "    tvals = train_scored[target].to_numpy()\n",
        "    vvals = valid_scored[target].to_numpy()\n",
        "    lo = float(np.percentile(np.concatenate([tvals, vvals]), 0.5))\n",
        "    hi = float(np.percentile(np.concatenate([tvals, vvals]), 99.5))\n",
        "    bins = np.linspace(lo, hi, 80)\n",
        "\n",
        "    axes[r, 0].hist(tvals, bins=bins, density=True, alpha=0.75, color='#1f77b4')\n",
        "    axes[r, 0].set_title(f'{target} train (scored)')\n",
        "    axes[r, 1].hist(vvals, bins=bins, density=True, alpha=0.75, color='#ff7f0e')\n",
        "    axes[r, 1].set_title(f'{target} valid (scored)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(ART_DIR / 'target_distributions.png', dpi=160)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "lags = [1, 2, 5, 10, 20, 50, 100]\n",
        "n_seq = train['seq_ix'].nunique()\n",
        "rows = []\n",
        "for target in TARGET_COLS:\n",
        "    arr = train[target].to_numpy().reshape(n_seq, 1000)\n",
        "    for lag in lags:\n",
        "        x = arr[:, :-lag].ravel()\n",
        "        y = arr[:, lag:].ravel()\n",
        "        rows.append({'target': target, 'lag': lag, 'autocorr': float(np.corrcoef(x, y)[0, 1])})\n",
        "\n",
        "autocorr_df = pd.DataFrame(rows)\n",
        "autocorr_df.to_csv(ART_DIR / 'target_autocorrelation.csv', index=False)\n",
        "display(autocorr_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4 \u2014 Feature-Target Relationships"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "ft_corr = train_scored[FEATURE_COLS + TARGET_COLS].corr().loc[FEATURE_COLS, TARGET_COLS]\n",
        "ft_corr.to_csv(ART_DIR / 'feature_target_pearson_corr.csv')\n",
        "\n",
        "for t in TARGET_COLS:\n",
        "    top = ft_corr[t].abs().sort_values(ascending=False).head(5)\n",
        "    print(f'Top 5 absolute Pearson for {t}:')\n",
        "    display(pd.DataFrame({'feature': top.index, 'pearson': [ft_corr.loc[f, t] for f in top.index]}))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rows = []\n",
        "for t in TARGET_COLS:\n",
        "    top = ft_corr[t].abs().sort_values(ascending=False).head(5).index\n",
        "    for feat in top:\n",
        "        rho, _ = spearmanr(train_scored[feat], train_scored[t])\n",
        "        rows.append({'target': t, 'feature': feat, 'pearson': float(ft_corr.loc[feat, t]), 'spearman': float(rho)})\n",
        "\n",
        "rank_df = pd.DataFrame(rows)\n",
        "rank_df.to_csv(ART_DIR / 'top_feature_pearson_vs_spearman.csv', index=False)\n",
        "display(rank_df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "scatter_n = 200_000\n",
        "sample_df = train_scored.iloc[RNG.choice(len(train_scored), size=min(scatter_n, len(train_scored)), replace=False)]\n",
        "\n",
        "for t in TARGET_COLS:\n",
        "    top_feats = ft_corr[t].abs().sort_values(ascending=False).head(5).index.tolist()\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "    axes = axes.flatten()\n",
        "    for i, feat in enumerate(top_feats):\n",
        "        ax = axes[i]\n",
        "        ax.scatter(sample_df[feat], sample_df[t], s=2, alpha=0.15)\n",
        "        ax.set_xlabel(feat)\n",
        "        ax.set_ylabel(t)\n",
        "        ax.set_title(f'{feat} vs {t}')\n",
        "    for j in range(len(top_feats), len(axes)):\n",
        "        axes[j].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(ART_DIR / f'scatter_top5_{t}.png', dpi=160)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5 \u2014 Temporal Structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "example_seq_ids = train['seq_ix'].drop_duplicates().sample(3, random_state=42).tolist()\n",
        "plot_cols = ['p0', 'p6', 'v0', 'v6', 'dp0', 'dv0', 't0', 't1']\n",
        "\n",
        "fig, axes = plt.subplots(len(plot_cols), len(example_seq_ids), figsize=(18, 20), sharex=True)\n",
        "for c, seq_id in enumerate(example_seq_ids):\n",
        "    seq_df = train[train['seq_ix'] == seq_id].sort_values('step_in_seq')\n",
        "    for r, col in enumerate(plot_cols):\n",
        "        ax = axes[r, c]\n",
        "        ax.plot(seq_df['step_in_seq'], seq_df[col], lw=0.8)\n",
        "        if r == 0:\n",
        "            ax.set_title(f'seq_ix={seq_id}')\n",
        "        if c == 0:\n",
        "            ax.set_ylabel(col)\n",
        "        if r == len(plot_cols) - 1:\n",
        "            ax.set_xlabel('step_in_seq')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(ART_DIR / 'example_sequences.png', dpi=160)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "selected_features = ['p0', 'p6', 'v0', 'v6', 'dp0', 'dv0']\n",
        "lags = [1, 2, 5, 10, 20, 50, 100]\n",
        "n_seq = train['seq_ix'].nunique()\n",
        "rows = []\n",
        "for feat in selected_features:\n",
        "    arr = train[feat].to_numpy().reshape(n_seq, 1000)\n",
        "    for lag in lags:\n",
        "        x = arr[:, :-lag].ravel()\n",
        "        y = arr[:, lag:].ravel()\n",
        "        rows.append({'feature': feat, 'lag': lag, 'autocorr': float(np.corrcoef(x, y)[0, 1])})\n",
        "\n",
        "feat_ac = pd.DataFrame(rows)\n",
        "feat_ac.to_csv(ART_DIR / 'selected_feature_autocorrelation.csv', index=False)\n",
        "display(feat_ac)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "stationarity_cols = ['p0', 'p6', 'v0', 'v6', 'dp0', 'dv0', 't0', 't1']\n",
        "step_stats = train.groupby('step_in_seq')[stationarity_cols].agg(['mean', 'std'])\n",
        "step_stats.to_csv(ART_DIR / 'stepwise_mean_std.csv')\n",
        "\n",
        "drift = {}\n",
        "for col in stationarity_cols:\n",
        "    step_mean = step_stats[(col, 'mean')]\n",
        "    drift[col] = float((step_mean.max() - step_mean.min()) / (train[col].std() + 1e-12))\n",
        "\n",
        "pd.DataFrame({'feature': list(drift.keys()), 'drift_ratio': list(drift.values())}).sort_values('drift_ratio', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6 \u2014 Cross-Feature Patterns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "derived = pd.DataFrame(index=train_scored.index)\n",
        "for i in range(6):\n",
        "    derived[f'spread_{i}'] = train_scored[f'p{6+i}'] - train_scored[f'p{i}']\n",
        "    b = train_scored[f'v{i}']\n",
        "    a = train_scored[f'v{6+i}']\n",
        "    derived[f'imbalance_{i}'] = (b - a) / (b + a + 1e-6)\n",
        "\n",
        "derived['bid_pressure'] = train_scored[[f'v{i}' for i in range(6)]].sum(axis=1)\n",
        "derived['ask_pressure'] = train_scored[[f'v{6+i}' for i in range(6)]].sum(axis=1)\n",
        "derived['pressure_imbalance'] = (derived['bid_pressure'] - derived['ask_pressure']) / (derived['bid_pressure'] + derived['ask_pressure'] + 1e-6)\n",
        "derived['trade_intensity'] = train_scored[[f'dv{i}' for i in range(4)]].sum(axis=1)\n",
        "\n",
        "rows = []\n",
        "for col in derived.columns:\n",
        "    x = derived[col].to_numpy()\n",
        "    for t in TARGET_COLS:\n",
        "        y = train_scored[t].to_numpy()\n",
        "        pear = np.corrcoef(x, y)[0, 1]\n",
        "        rho, _ = spearmanr(x, y)\n",
        "        rows.append({'feature': col, 'target': t, 'pearson': float(pear), 'spearman': float(rho)})\n",
        "\n",
        "derived_corr = pd.DataFrame(rows)\n",
        "derived_corr['abs_pearson'] = derived_corr['pearson'].abs()\n",
        "derived_corr = derived_corr.sort_values(['target', 'abs_pearson'], ascending=[True, False])\n",
        "derived_corr.to_csv(ART_DIR / 'derived_feature_target_corr.csv', index=False)\n",
        "\n",
        "print('Top derived for t0:')\n",
        "display(derived_corr[derived_corr['target'] == 't0'].head(5))\n",
        "print('Top derived for t1:')\n",
        "display(derived_corr[derived_corr['target'] == 't1'].head(5))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "main_derived = ['spread_0', 'spread_1', 'imbalance_0', 'imbalance_1', 'pressure_imbalance', 'trade_intensity']\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 8))\n",
        "axes = axes.flatten()\n",
        "for i, col in enumerate(main_derived):\n",
        "    vals = derived[col].to_numpy()\n",
        "    lo = float(np.percentile(vals, 0.5))\n",
        "    hi = float(np.percentile(vals, 99.5))\n",
        "    bins = np.linspace(lo, hi, 80)\n",
        "    axes[i].hist(vals, bins=bins, density=True, alpha=0.75, color='#2ca02c')\n",
        "    axes[i].set_title(col)\n",
        "plt.tight_layout()\n",
        "plt.savefig(ART_DIR / 'derived_feature_distributions.png', dpi=160)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Answers (for engineering decisions)\n",
        "\n",
        "- **Normalization needed**: cross-feature std ratio is large; use train-fit global z-score baseline and test per-sequence centering ablation.\n",
        "- **Most predictive raw features**: price features dominate for `t0`; volume/trade features dominate for `t1` but with weaker linear correlation.\n",
        "- **Derived features help**: spread signals improve `t0` linear signal modestly; limited linear gain for `t1`.\n",
        "- **Targets are related**: moderate positive `corr(t0, t1)` suggests shared backbone + separate heads is appropriate.\n",
        "- **Temporal structure is strong**: especially `t1`, indicating sequence memory is critical and longer-context modeling should help.\n",
        "- **Train/valid shift exists**: notable distribution shift in several price features (`p0`, `p6`, `p7`, etc.) requiring robust validation and normalization hygiene.\n",
        "- **Metric-weight concentration**: top 10\u201320% of |target| contributes a large share of weight, so loss weighting for large moves is justified."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}