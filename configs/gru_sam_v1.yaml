# SAM (Sharpness-Aware Minimization) for better generalization
# Base: gru_derived_tightwd_v2 with optimizer changed to SAM
# SAM finds flatter loss minima that generalize better to test data
# Hypothesis: our val-LB gap is due to sharp minima; SAM should reduce it
# AMP disabled â€” SAM needs fp32 for the two-step perturbation
# Kill test: 3 seeds (s42/s43/s44), pass if mean val >= 0.2650
#   (lower threshold: SAM may trade val for better generalization)

model:
  type: gru
  input_size: 42
  hidden_size: 144
  num_layers: 2
  dropout: 0.22
  output_size: 2

training:
  optimizer: sam
  sam_rho: 0.05
  sam_base_optimizer: adamw
  lr: 0.0008
  weight_decay: 5e-5
  epochs: 35
  batch_size: 192
  gradient_clip: 1.0
  early_stopping_patience: 8
  loss: combined
  weighted_ratio: 0.62
  use_amp: false
  scheduler:
    type: reduce_on_plateau
    factor: 0.5
    patience: 3
    min_lr: 1e-6

data:
  train_path: datasets/train.parquet
  valid_path: datasets/valid.parquet
  normalize: true
  derived_features: true

evaluation:
  clip_predictions: true
  clip_range: [-6, 6]

logging:
  log_dir: logs
  save_every: 5
