# SSL PRETRAINING CONFIG
# Stage 1 of two-stage SSL: masked timestep reconstruction pretrain.
# Identical to gru_parity_v1 but with ssl_pretrain: true (adds reconstruction head).
#
# Usage:
#   python scripts/train_ssl.py --config configs/gru_ssl_pretrain_v1.yaml --seed 42 --device cuda
#
# Output: logs/gru_ssl_pretrain_v1_seed42.pt (ssl head excluded, ready for --resume)
#
# Stage 2 fine-tune:
#   python scripts/train.py --config configs/gru_parity_v1.yaml --seed 42 --device cuda \
#       --resume logs/gru_ssl_pretrain_v1_seed42.pt

model:
  type: gru
  input_size: 32
  hidden_size: 64
  num_layers: 3
  dropout: 0.0
  output_size: 2
  vanilla: true
  output_type: linear
  ssl_pretrain: true

training:
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0
  epochs: 15
  batch_size: 192
  gradient_clip: 1.0
  ssl_mask_ratio: 0.25
  use_amp: true

data:
  train_path: datasets/train.parquet
  valid_path: datasets/valid.parquet
  normalize: false
  derived_features: false

logging:
  log_dir: logs
