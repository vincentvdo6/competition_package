# GRU refined v3 â€” exploring around the tightwd_v2 sweet spot
# tightwd_v2 scored 0.2674. This config nudges parameters in promising directions:
# - hidden_size 144 (same, proven)
# - dropout 0.25 (slightly higher for more regularization)
# - weight_decay 6e-5 (slightly higher, tightwd trend suggests more WD helps)
# - lr 0.0007 (slightly lower, slower convergence but potentially better peak)
# - weighted_ratio 0.65 (slightly more WeightedMSE emphasis)

model:
  type: gru
  input_size: 42
  hidden_size: 144
  num_layers: 2
  dropout: 0.25
  output_size: 2

training:
  lr: 0.0007
  weight_decay: 6e-5
  epochs: 40
  batch_size: 192
  gradient_clip: 1.0
  early_stopping_patience: 10
  loss: combined
  weighted_ratio: 0.65
  use_amp: true
  scheduler:
    type: reduce_on_plateau
    factor: 0.5
    patience: 4
    min_lr: 1e-6

data:
  train_path: datasets/train.parquet
  valid_path: datasets/valid.parquet
  normalize: true
  derived_features: true

evaluation:
  clip_predictions: true
  clip_range: [-6, 6]

logging:
  log_dir: logs
  save_every: 5
